# =============================================================================
# DIVE V3 - Hub Stack (OPAL + Core Services)
# =============================================================================
# Purpose:
#   Minimal hub footprint for policy distribution (OPAL Server) plus the
#   supporting identity/policy services required to serve policy data.
# Usage:
#   ./dive hub deploy       # uses this compose file
#   ./dive hub up           # start services
# Notes:
#   - Secrets come from ./dive load_secrets (GCP or local defaults).
#   - Certificates are expected in instances/hub/certs (SSOT via ./dive certs).
#   - No spoke/partner services are included here.
# =============================================================================

name: dive-hub

networks:
  hub-internal:
    driver: bridge
  # Shared network for local dev only (when hub + spokes on same server)
  # In production, instances communicate via external domains
  # Federation shared network for cross-instance communication (OPAL, Keycloak)
  dive-shared:
    external: true

volumes:
  postgres_data:
  mongodb_data:
  redis_data:
  redis_blacklist_data:
  frontend_node_modules:
  frontend_next:
  authzforce_data:

services:
  # ---------------------------------------------------------------------------
  # PostgreSQL - backing store for Keycloak / NextAuth
  # ---------------------------------------------------------------------------
  postgres:
    image: postgres:15-alpine
    container_name: ${COMPOSE_PROJECT_NAME}-postgres
    restart: unless-stopped
    env_file:
      - .env.hub
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: keycloak_db
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/setup/init-db.sh:/docker-entrypoint-initdb.d/init-db.sh
      - ./scripts/postgres-init:/scripts/postgres-init:ro
    networks:
      - hub-internal
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ---------------------------------------------------------------------------
  # MongoDB - policy/resource data backing store for backend
  # ---------------------------------------------------------------------------
  mongodb:
    image: mongo:8.0.17
    container_name: ${COMPOSE_PROJECT_NAME}-mongodb
    restart: unless-stopped
    env_file:
      - .env.hub
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_PASSWORD}
      MONGO_INITDB_DATABASE: dive-v3
    volumes:
      - mongodb_data:/data/db
    networks:
      - hub-internal
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 10s
      timeout: 5s
      retries: 5

  # ---------------------------------------------------------------------------
  # Redis - session/blacklist cache (Hardened)
  # ---------------------------------------------------------------------------
  redis:
    image: redis:7-alpine
    container_name: ${COMPOSE_PROJECT_NAME}-redis
    restart: unless-stopped
    env_file:
      - .env.hub
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD_USA}
      --appendonly yes
      --appendfsync everysec
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --tcp-keepalive 300
      --timeout 0
      --loglevel notice
      --save 900 1
      --save 300 10
      --save 60 10000
    environment:
      REDIS_PASSWORD_USA: ${REDIS_PASSWORD_USA}
    networks:
      - hub-internal
      - dive-shared
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD_USA}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # ---------------------------------------------------------------------------
  # Redis Blacklist - Shared token revocation across all instances (GAP-010)
  # ---------------------------------------------------------------------------
  redis-blacklist:
    image: redis:7-alpine
    container_name: ${COMPOSE_PROJECT_NAME}-redis-blacklist
    restart: unless-stopped
    env_file:
      - .env.hub
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD_BLACKLIST}
      --appendonly yes
      --appendfsync everysec
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --tcp-keepalive 300
      --timeout 0
      --loglevel notice
      --save 900 1
      --save 300 10
      --save 60 10000
    environment:
      REDIS_PASSWORD_BLACKLIST: ${REDIS_PASSWORD_BLACKLIST}
    networks:
      - hub-internal
      - dive-shared
    volumes:
      - redis_blacklist_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD_BLACKLIST}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # ---------------------------------------------------------------------------
  # Redis Exporter - Prometheus metrics

  # ---------------------------------------------------------------------------
  # Keycloak - Identity Broker
  # ---------------------------------------------------------------------------
  keycloak:
    build:
      context: ./keycloak
      dockerfile: Dockerfile
    container_name: ${COMPOSE_PROJECT_NAME}-keycloak
    restart: unless-stopped
    env_file:
      - .env.hub
    entrypoint: ["/bin/bash", "/opt/keycloak/scripts/import-realm.sh"]
    command: ["start-dev", "--spi-login-protocol-openid-connect-suppress-logout-confirmation-screen=true", "--features=scripts"]
    environment:
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://postgres:5432/keycloak_db
      KC_DB_USERNAME: postgres
      KC_DB_PASSWORD: ${POSTGRES_PASSWORD}
      KC_HOSTNAME: ${KEYCLOAK_HOSTNAME:-localhost}
      KC_HOSTNAME_STRICT: "false"
      KC_PROXY_HEADERS: xforwarded
      KC_HTTP_ENABLED: "true"
      KC_HTTPS_CERTIFICATE_FILE: /opt/keycloak/certs/certificate.pem
      KC_HTTPS_CERTIFICATE_KEY_FILE: /opt/keycloak/certs/key.pem
      KC_HTTPS_PORT: "8443"
      # Trust mkcert root CA for federation with spoke Keycloaks
      KC_TRUSTSTORE_PATHS: /opt/keycloak/certs/mkcert-rootCA.pem
      KC_LOG_LEVEL: info
      KC_METRICS_ENABLED: "true"
      KC_HEALTH_ENABLED: "true"
      KC_FEATURES: scripts
      KC_ADMIN: admin
      KC_ADMIN_PASSWORD: ${KC_ADMIN_PASSWORD}
      # Initial admin user creation (Keycloak 26.5.0)
      KC_BOOTSTRAP_ADMIN_USERNAME: admin
      KC_BOOTSTRAP_ADMIN_PASSWORD: ${KC_ADMIN_PASSWORD}
      # Bootstrap realm for Terraform provider authentication
      SKIP_REALM_IMPORT: "true"  # Terraform SSOT: Start empty, configure via Terraform
      # Legacy realm import variables (kept for compatibility, but not used)
      KEYCLOAK_CLIENT_SECRET: ${KEYCLOAK_CLIENT_SECRET:?KEYCLOAK_CLIENT_SECRET required}
      APP_URL: ${NEXT_PUBLIC_BASE_URL:-https://localhost:3000}
      API_URL: ${NEXT_PUBLIC_API_URL:-https://localhost:4000}
      USA_IDP_URL: ${USA_IDP_URL:-https://keycloak:8443}
      USA_IDP_CLIENT_SECRET: ${USA_IDP_CLIENT_SECRET:-default-usa-idp-secret}
      ADMIN_PASSWORD: ${ADMIN_PASSWORD:-DiveAdminSecure2025!}
      TEST_USER_PASSWORD: ${TEST_USER_PASSWORD:-DiveTestSecure2025!}
      INSTANCE_CODE: ${INSTANCE:-USA}
    ports:
      - "127.0.0.1:${KEYCLOAK_HTTP_PORT:-8080}:8080"
      - "127.0.0.1:${KEYCLOAK_HTTPS_PORT:-8443}:8443"
      - "127.0.0.1:${KEYCLOAK_MGMT_PORT:-9000}:9000"
    volumes:
      - ./instances/hub/certs:/opt/keycloak/certs:ro
      - ./keycloak/themes:/opt/keycloak/themes:ro
      # ADDED (Dec 2025): Mount scripts to pick up local fixes without rebuilding image
      - ./keycloak/scripts:/opt/keycloak/scripts:ro
    networks:
      - hub-internal
      - dive-shared  # Required for federation (Keycloak needs to reach spoke Keycloaks)
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -ksf https://localhost:8443/realms/master >/dev/null"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  # ---------------------------------------------------------------------------
  # OPA - Policy Decision Point
  # ---------------------------------------------------------------------------
  opa:
    image: openpolicyagent/opa:1.12.1
    container_name: ${COMPOSE_PROJECT_NAME}-opa
    restart: unless-stopped
    command:
      - run
      - --server
      - --addr=0.0.0.0:8181
      - --tls-cert-file=/certs/certificate.pem
      - --tls-private-key-file=/certs/key.pem
      - --bundle
      - /policies
      - --log-level=info
    ports:
      - "127.0.0.1:${OPA_PORT:-8181}:8181"
      - "127.0.0.1:${OPA_METRICS_PORT:-8182}:8182"
    volumes:
      - ./policies:/policies:ro
      - ./instances/hub/certs:/certs:ro
    networks:
      - hub-internal
    healthcheck:
      test: ["CMD", "/opa", "version"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # ---------------------------------------------------------------------------
  # Backend API - serves policy data for OPAL and Hub admin APIs
  # ---------------------------------------------------------------------------
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: ${COMPOSE_PROJECT_NAME}-backend
    restart: unless-stopped
    env_file:
      - .env.hub
    environment:
      NODE_ENV: development
      PORT: "4000"
      MONGODB_URL: mongodb://admin:${MONGO_PASSWORD}@mongodb:27017
      MONGODB_DATABASE: dive-v3
      OPA_URL: https://opa:8181
      KEYCLOAK_URL: https://keycloak:8443
      KEYCLOAK_REALM: dive-v3-broker-usa
      KEYCLOAK_CLIENT_ID: ${KEYCLOAK_CLIENT_ID:-dive-v3-broker-usa}
      KEYCLOAK_CLIENT_SECRET: ${KEYCLOAK_CLIENT_SECRET}
      KEYCLOAK_JWKS_URI: https://keycloak:8443/realms/dive-v3-broker-usa/protocol/openid-connect/certs
      KEYCLOAK_ADMIN_USERNAME: admin
      KC_ADMIN_PASSWORD: ${KC_ADMIN_PASSWORD}
      # Redis configuration
      REDIS_PASSWORD: ${REDIS_PASSWORD_USA}
      REDIS_URL: redis://:${REDIS_PASSWORD_USA}@redis:6379
      # Shared blacklist Redis
      BLACKLIST_REDIS_URL: redis://:${REDIS_PASSWORD_BLACKLIST}@redis-blacklist:6379
      # NOTE: Spoke Keycloak admin passwords come from spoke registration, NOT environment
      # The spoke provides its password during registration, stored in MongoDB
      NEXT_PUBLIC_BASE_URL: https://localhost:3000
      # SECURITY: Trust mkcert CA instead of disabling TLS verification
      NODE_EXTRA_CA_CERTS: /app/certs/ca/rootCA.pem
      OPAL_SERVER_URL: https://opal-server:7002
      OPAL_DATA_TOPICS: policy_data
      # Docker exec configuration for running OPA tests
      OPA_CONTAINER: ${COMPOSE_PROJECT_NAME}-opa
      OPA_POLICIES_PATH: /policies
      # KAS URL for key requests (use container name since KAS is manually started)
      KAS_URL: https://kas:8080
      # Enable polling for file watching in Docker (fixes hot-reload)
      CHOKIDAR_USEPOLLING: "true"
      CHOKIDAR_INTERVAL: "1000"
    ports:
      - "127.0.0.1:${BACKEND_PORT:-4000}:4000"
    volumes:
      - ./backend/src:/app/src
      - ./backend/package.json:/app/package.json:ro
      - ./backend/tsconfig.json:/app/tsconfig.json:ro
      - ./instances/hub/certs:/opt/keycloak/certs:ro
      - ./policies:/app/policies:ro
      - ./backend/certs:/app/certs:ro
      - ./certs/mkcert:/app/certs/ca:ro  # mkcert root CA for TLS trust
      - ./backend/logs:/app/logs
      - ./config:/app/config:ro
      # Docker socket for running OPA tests via docker exec
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - hub-internal
      - dive-shared
    depends_on:
      keycloak:
        condition: service_healthy
      mongodb:
        condition: service_healthy
      redis:
        condition: service_healthy
      redis-blacklist:
        condition: service_healthy
      opa:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget --no-check-certificate -qO- https://localhost:4000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ---------------------------------------------------------------------------
  # AuthzForce - XACML Policy Decision Point (optional, for XACML compliance)
  # ---------------------------------------------------------------------------
  # NOTE: AuthzForce runs HTTP internally. This is secure because:
  #   1. Only accessible within the Docker network (hub-internal)
  #   2. All external access goes through HTTPS backend/frontend
  #   3. No sensitive data exposed externally on port 8282
  # To enable HTTPS, use a reverse proxy (nginx/traefik) in front of AuthzForce
  # ---------------------------------------------------------------------------
  authzforce:
    image: authzforce/server:12.0.1
    container_name: ${COMPOSE_PROJECT_NAME}-authzforce
    restart: unless-stopped
    # NOTE: Exposing port for debugging only. In production, remove this.
    ports:
      - "127.0.0.1:${AUTHZFORCE_PORT:-8282}:8080"
    volumes:
      - ./authzforce/conf:/opt/authzforce-ce-server/conf:ro
      - authzforce_data:/opt/authzforce-ce-server/data
      - ./policies/uploads:/policies:ro
    environment:
      JAVA_OPTS: -Xms256m -Xmx512m
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/authzforce-ce/domains"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - hub-internal

  # ---------------------------------------------------------------------------
  # KAS - Key Access Service (Stretch Goal)
  # ---------------------------------------------------------------------------
  kas:
    build:
      context: ./kas
      dockerfile: Dockerfile
    container_name: ${COMPOSE_PROJECT_NAME}-kas
    restart: unless-stopped
    env_file:
      - .env.hub
    healthcheck:
      test: ["CMD-SHELL", "wget --no-check-certificate -q -O- https://localhost:8080/health || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      NODE_ENV: development
      KAS_PORT: 8080
      HTTPS_ENABLED: "true"
      CERT_PATH: /opt/app/certs
      KEY_FILE: key.pem
      CERT_FILE: certificate.pem
      BACKEND_URL: https://backend:4000
      # SECURITY: Trust mkcert CA instead of disabling TLS verification
      NODE_EXTRA_CA_CERTS: /app/certs/ca/rootCA.pem
      OPA_URL: https://opa:8181
      MONGODB_URL: mongodb://admin:${MONGO_PASSWORD}@mongodb:27017?authSource=admin
      MONGODB_DATABASE: dive-v3
      LOG_LEVEL: info
      KEYCLOAK_URL: https://keycloak:8443
      KEYCLOAK_REALM: dive-v3-broker-usa
      KEYCLOAK_CLIENT_ID: ${KEYCLOAK_CLIENT_ID:-dive-v3-broker-usa}
    ports:
      - "127.0.0.1:8085:8080"
    depends_on:
      - opa
      - mongodb
    networks:
      - hub-internal
    volumes:
      - ./kas/certs:/opt/app/certs:ro
      - ./certs/mkcert:/app/certs/ca:ro  # mkcert root CA for TLS trust
      - ./kas/logs:/app/logs
      - ./config:/app/config:ro
    # Use built version instead of dev mode
    command: node dist/server.js

  # ---------------------------------------------------------------------------
  # Frontend - Next.js Application
  # ---------------------------------------------------------------------------
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    container_name: ${COMPOSE_PROJECT_NAME}-frontend
    restart: unless-stopped
    env_file:
      - .env.hub
    healthcheck:
      test: ["CMD-SHELL", "curl -ksf https://localhost:3000/ >/dev/null || exit 1"]
      interval: 30s
      timeout: 15s
      retries: 10
      start_period: 120s
    environment:
      NODE_ENV: development
      NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL:-https://localhost:4000}
      NEXT_PUBLIC_BACKEND_URL: ${NEXT_PUBLIC_BACKEND_URL:-https://localhost:4000}
      NEXT_PUBLIC_BASE_URL: ${NEXT_PUBLIC_BASE_URL:-https://localhost:3000}
      BACKEND_URL: https://backend:4000
      KEYCLOAK_BASE_URL: https://keycloak:8443
      KEYCLOAK_URL: ${KEYCLOAK_URL:-https://localhost:8443}
      KEYCLOAK_REALM: ${KEYCLOAK_REALM:-dive-v3-broker-usa}
      NEXT_PUBLIC_KEYCLOAK_URL: ${NEXT_PUBLIC_KEYCLOAK_URL:-https://localhost:8443}
      NEXT_PUBLIC_KEYCLOAK_REALM: ${NEXT_PUBLIC_KEYCLOAK_REALM:-dive-v3-broker-usa}
      KEYCLOAK_ISSUER: ${KEYCLOAK_ISSUER:-https://localhost:8443/realms/dive-v3-broker-usa}
      NEXTAUTH_URL: ${NEXTAUTH_URL:-https://localhost:3000}
      NEXT_PUBLIC_EXTERNAL_DOMAINS: "${NEXT_PUBLIC_EXTERNAL_DOMAINS:-https://localhost:3000,https://localhost:4000,https://localhost:8443}"
      AUTH_SECRET: ${AUTH_SECRET:-default-auth-secret-change-me}
      NEXTAUTH_SECRET: ${AUTH_SECRET:-default-auth-secret-change-me}
      KEYCLOAK_CLIENT_ID: ${KEYCLOAK_CLIENT_ID:-dive-v3-broker-usa}
      KEYCLOAK_CLIENT_SECRET: ${KEYCLOAK_CLIENT_SECRET}
      AUTH_KEYCLOAK_ID: ${KEYCLOAK_CLIENT_ID:-dive-v3-broker-usa}
      AUTH_KEYCLOAK_SECRET: ${KEYCLOAK_CLIENT_SECRET}
      AUTH_KEYCLOAK_ISSUER: ${AUTH_KEYCLOAK_ISSUER:-https://localhost:8443/realms/dive-v3-broker-usa}
      AUTH_TRUST_HOST: "true"
      # TLS Certificate Handling - Proper mkcert Integration (2026 Best Practice)
      # Makes Node.js fetch() API use system CA trust store (mkcert CA installed via entrypoint)
      # See: docs/TLS_BEST_PRACTICES_2026.md
      NODE_OPTIONS: "--use-openssl-ca"
      NODE_EXTRA_CA_CERTS: /app/certs/ca/rootCA.pem  # Legacy https module support
      DATABASE_URL: postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/dive_v3_app
    ports:
      - "127.0.0.1:${FRONTEND_PORT:-3000}:3000"
    depends_on:
      keycloak:
        condition: service_healthy
      backend:
        condition: service_healthy
    networks:
      - hub-internal
    extra_hosts:
      - "localhost:host-gateway"
    volumes:
      - ./frontend:/app
      - ./instances/hub/certs:/opt/keycloak/certs:ro
      - ./frontend/certs:/opt/app/certs:ro
      - ./certs/mkcert:/app/certs/ca:ro  # mkcert root CA for TLS trust
      - frontend_node_modules:/app/node_modules
      - frontend_next:/app/.next

  # ---------------------------------------------------------------------------
  # OPAL Server - Policy distribution hub
  # ---------------------------------------------------------------------------
  opal-server:
    build:
      context: ./docker
      dockerfile: opal-server.Dockerfile
    container_name: ${COMPOSE_PROJECT_NAME}-opal-server
    restart: unless-stopped
    stop_grace_period: 30s
    env_file:
      - .env.hub
    environment:
      # Use Redis as broadcast backend for pub/sub
      OPAL_BROADCAST_URI: redis://:${REDIS_PASSWORD_USA}@redis:6379
      # Git-based policy distribution - policies from GitHub repo
      OPAL_REPO_WATCHER_ENABLED: "true"
      OPAL_POLICY_REPO_URL: https://github.com/albeach/dive-v3-policies.git
      OPAL_POLICY_REPO_MAIN_BRANCH: master
      OPAL_POLICY_REPO_POLLING_INTERVAL: 30
      # Policy paths to include (relative to repo root)
      # CRITICAL FIX (2026-01-15): Include root (.) for admin policies and other root-level files
      # Without root, files like admin_authorization_policy.rego are excluded
      OPAL_POLICY_SOURCE_DIRS: .,base,org,tenant,entrypoints,compat
      # Multiple data topics for real-time policy data distribution (Phase 2)
      OPAL_DATA_TOPICS_DEFAULT: policy_data,trusted_issuers,federation_matrix,tenant_configs
      OPAL_INLINE_OPA_CONFIG: "false"
      OPAL_LOG_LEVEL: DEBUG
      OPAL_LOG_FORMAT: text
      UVICORN_PORT: 7002
      UVICORN_HOST: 0.0.0.0
      OPAL_SERVER_WORKERS: 1
      UVICORN_SSL_CERTFILE: /certs/certificate.pem
      UVICORN_SSL_KEYFILE: /certs/key.pem
      OPAL_STATISTICS_ENABLED: "true"
      OPAL_SERVER_CORS_ALLOWED_ORIGINS: '["http://localhost:3000","https://localhost:3000","http://localhost:7002"]'
      # Data source URLs with multiple topics (Phase 2: Real-Time Policy Data)
      # Each endpoint serves dynamic data from MongoDB (with file fallback)
      # - policy_data: Combined policy data (legacy, for compatibility)
      # - trusted_issuers: Dynamic trusted IdP issuers from MongoDB
      # - federation_matrix: Dynamic federation trust relationships
      # - tenant_configs: Per-nation tenant configurations
      OPAL_DATA_CONFIG_SOURCES: |
        {
          "config": {
            "entries": [
              {
                "url": "https://host.docker.internal:4000/api/opal/policy-data",
                "topics": ["policy_data"],
                "dst_path": "dive/federation",
                "config": {"headers": {"Accept": "application/json"}}
              },
              {
                "url": "https://host.docker.internal:4000/api/opal/trusted-issuers",
                "topics": ["trusted_issuers"],
                "dst_path": "trusted_issuers",
                "config": {"headers": {"Accept": "application/json"}}
              },
              {
                "url": "https://host.docker.internal:4000/api/opal/federation-matrix",
                "topics": ["federation_matrix"],
                "dst_path": "federation_matrix",
                "config": {"headers": {"Accept": "application/json"}}
              },
              {
                "url": "https://host.docker.internal:4000/api/opal/tenant-configs",
                "topics": ["tenant_configs"],
                "dst_path": "tenant_configs",
                "config": {"headers": {"Accept": "application/json"}}
              }
            ]
          }
        }
      # OPAL Authentication - JWT token verification for spoke clients
      OPAL_AUTH_MASTER_TOKEN: ${OPAL_AUTH_MASTER_TOKEN}
    ports:
      - "127.0.0.1:${OPAL_PORT:-7002}:7002"
    networks:
      - hub-internal
      - dive-shared  # Allow spoke OPAL clients to connect via federation network
    volumes:
      # NOTE: ./policies volume mount removed to prevent duplication with Git-based distribution
      # OPAL fetches policies from GitHub repo: https://github.com/albeach/dive-v3-policies.git
      - ./opal-data-source:/opal-data-source:ro
      - ./instances/hub/certs:/certs:ro
      - ./certs/opal:/opal-keys:ro
    # Uses custom entrypoint from Dockerfile that loads auth keys
    healthcheck:
      test: ["CMD", "curl", "-k", "-f", "https://localhost:7002/healthcheck"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 60s
    depends_on:
      - opa
      - backend
