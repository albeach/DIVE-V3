# =============================================================================
# DIVE V3 - Hub Stack (OPAL + Core Services)
# =============================================================================
# Purpose:
#   Minimal hub footprint for policy distribution (OPAL Server) plus the
#   supporting identity/policy services required to serve policy data.
# Usage:
#   ./dive hub deploy       # uses this compose file (loads env automatically)
#   ./dive hub up           # start services
#
#   Manual usage (when not using dive CLI):
#   RECOMMENDED: Create symlink for Docker Compose default behavior
#   ln -sf .env.hub .env
#   docker compose -f docker-compose.hub.yml up
#
#   OR: Use --env-file flag (all commands must include it):
#   docker compose -f docker-compose.hub.yml --env-file .env.hub up
#   docker compose -f docker-compose.hub.yml --env-file .env.hub restart backend
#
# Environment Variable Loading (Docker Compose Behavior):
#   Docker Compose interpolates ${VARIABLES} in TWO phases:
#   1. PARSE PHASE: Loads .env (default filename) OR --env-file for ${VAR} interpolation
#   2. CONTAINER PHASE: Loads service-level env_file: entries into container environment
#
#   CRITICAL: ${MONGO_PASSWORD} in this file needs to exist in PARSE PHASE:
#   - Option A (Recommended): ln -sf .env.hub .env
#   - Option B: Always use --env-file .env.hub flag
#   - Option C: Export vars to shell: export $(cat .env.hub | xargs)
#
# Notes:
#   - Secrets come from ./dive load_secrets (GCP or local defaults).
#   - Certificates are expected in instances/hub/certs (SSOT via ./dive certs).
#   - No spoke/partner services are included here.
# =============================================================================

name: dive-hub

networks:
  hub-internal:
    driver: bridge
    labels:
      dive.network.type: "internal"
      dive.network.scope: "hub"
      dive.network.description: "Hub internal service communication"
  # Shared network for federation, monitoring, and cross-instance communication
  # Used by: Hub â†” Spokes, Monitoring (Prometheus/Grafana), Shared Blacklist
  # In production, instances communicate via external domains
  dive-shared:
    external: true

volumes:
  postgres_data:
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "postgres"
      dive.resource.purpose: "database"
  mongodb_data:
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "mongodb"
      dive.resource.purpose: "database"
  mongodb_config:  # MongoDB replica set configuration
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "mongodb"
      dive.resource.purpose: "config"
  redis_data:
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "redis"
      dive.resource.purpose: "cache"
  # redis_blacklist_data: MOVED to shared stack (shared-blacklist-redis-data)
  frontend_node_modules:  # Development hot reload
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "frontend"
      dive.resource.purpose: "node_modules"
  frontend_next:  # Development hot reload
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "frontend"
      dive.resource.purpose: "build_cache"
  authzforce_data:
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "authzforce"
      dive.resource.purpose: "data"
  authzforce_server:        # Prevent anonymous volume from base image
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "authzforce"
      dive.resource.purpose: "server"
  authzforce_tomcat_conf:   # Prevent anonymous volume from base image
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "authzforce"
      dive.resource.purpose: "config"

services:
  # ---------------------------------------------------------------------------
  # PostgreSQL - backing store for Keycloak / NextAuth
  # ---------------------------------------------------------------------------
  postgres:
    labels:
      dive.service.class: "core"
      dive.service.description: "PostgreSQL database for Keycloak user/realm storage"
    image: postgres:18.1-alpine3.23
    container_name: ${COMPOSE_PROJECT_NAME}-postgres
    restart: unless-stopped
    env_file:
      - .env.hub
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: keycloak_db
    volumes:
      - postgres_data:/var/lib/postgresql  # Mount parent dir to prevent anonymous volume
      - ./scripts/setup/init-db.sh:/docker-entrypoint-initdb.d/init-db.sh
      - ./scripts/postgres-init:/scripts/postgres-init:ro
    networks:
      - hub-internal
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 15s

  # ---------------------------------------------------------------------------
  # MongoDB - policy/resource data backing store for backend
  # CONFIGURED AS SINGLE-NODE REPLICA SET for OPAL CDC change streams
  # ---------------------------------------------------------------------------
  mongodb:
    labels:
      dive.service.class: "core"
      dive.service.description: "MongoDB database for resource metadata and audit logs"
    image: mongo:8.0.17
    container_name: ${COMPOSE_PROJECT_NAME}-mongodb
    restart: unless-stopped
    # Production-grade replica set: copy keyFile to writable location with proper permissions
    entrypoint: >
      bash -c "
        echo 'ðŸ” Preparing MongoDB replica set with keyFile authentication'
        cp /data/keyfile/mongo-keyfile /tmp/mongo-keyfile
        chmod 400 /tmp/mongo-keyfile
        chown 999:999 /tmp/mongo-keyfile
        echo 'âœ… KeyFile configured at /tmp/mongo-keyfile'
        exec /usr/local/bin/docker-entrypoint.sh mongod --replSet rs0 --keyFile /tmp/mongo-keyfile --bind_ip_all
      "
    env_file:
      - .env.hub
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_PASSWORD}
      MONGO_INITDB_DATABASE: dive-v3
    volumes:
      - mongodb_data:/data/db
      - mongodb_config:/data/configdb
      # Mount replica set keyFile (will be copied to /tmp with correct permissions)
      - ./instances/hub/mongo-keyfile:/data/keyfile/mongo-keyfile:ro
      # NOTE: Replica set initialization happens POST-START via deployment script
      # docker-entrypoint-initdb.d/ runs BEFORE --replSet is applied, so it cannot work
      # See: scripts/init-mongo-replica-set-post-start.sh (called by hub_deploy())
    networks:
      - hub-internal
    healthcheck:
      # Check if MongoDB is accepting connections (basic connectivity only)
      # Replica set initialization happens in Phase 4a of deployment
      test: >
        mongosh admin -u admin -p ${MONGO_PASSWORD} --quiet --eval "db.adminCommand('ping')" | grep -q "ok"
      interval: 3s
      timeout: 5s
      retries: 15
      start_period: 20s

  # ---------------------------------------------------------------------------
  # Redis - session/blacklist cache (Hardened)
  # ---------------------------------------------------------------------------
  redis:
    labels:
      dive.service.class: "core"
      dive.service.description: "Redis cache for session state and policy decisions"
    image: redis:7-alpine
    container_name: ${COMPOSE_PROJECT_NAME}-redis
    restart: unless-stopped
    env_file:
      - .env.hub
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD_USA}
      --appendonly yes
      --appendfsync everysec
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --tcp-keepalive 300
      --timeout 0
      --loglevel notice
      --save 900 1
      --save 300 10
      --save 60 10000
    environment:
      REDIS_PASSWORD_USA: ${REDIS_PASSWORD_USA}
    ports:
      # Changed from 6379:6379 to avoid conflict with local redis-server
      # Internal Docker network still uses :6379, only host port changed
      - "127.0.0.1:6380:6379"
    networks:
      - hub-internal
      - dive-shared
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD_USA}", "ping"]
      interval: 3s
      timeout: 3s
      retries: 5
      start_period: 5s

  # ---------------------------------------------------------------------------
  # Token Revocation Store - CENTRALIZED IN SHARED STACK
  # ---------------------------------------------------------------------------
  # Token revocation is now in docker/instances/shared/docker-compose.yml
  # Service name: shared-token-store (was: shared-blacklist-redis)
  # Network: dive-shared
  # Rationale: Cross-instance token revocation for federation (USA/FRA/GBR/DEU)
  #
  # Backend connects via: shared-token-store:6379
  # Requires: shared stack running (`cd docker/instances/shared && docker compose up -d`)
  # ---------------------------------------------------------------------------

  # ---------------------------------------------------------------------------
  # Redis Exporter - Prometheus metrics

  # ---------------------------------------------------------------------------
  # Keycloak - Identity Broker
  # ---------------------------------------------------------------------------
  keycloak:
    labels:
      dive.service.class: "core"
      dive.service.description: "Keycloak IdP broker for multi-nation SSO"
    build:
      context: ./keycloak
      dockerfile: Dockerfile
    container_name: ${COMPOSE_PROJECT_NAME}-keycloak
    restart: unless-stopped
    env_file:
      - .env.hub
    entrypoint: ["/bin/bash", "/opt/keycloak/scripts/import-realm.sh"]
    command: ["start-dev", "--spi-login-protocol-openid-connect-suppress-logout-confirmation-screen=true", "--features=scripts"]
    environment:
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://postgres:5432/keycloak_db
      KC_DB_USERNAME: postgres
      KC_DB_PASSWORD: ${POSTGRES_PASSWORD}
      KC_HOSTNAME: ${KEYCLOAK_HOSTNAME:-localhost}
      KC_HOSTNAME_STRICT: "false"
      KC_PROXY_HEADERS: xforwarded
      KC_HTTP_ENABLED: "true"
      KC_HTTPS_CERTIFICATE_FILE: /opt/keycloak/certs/certificate.pem
      KC_HTTPS_CERTIFICATE_KEY_FILE: /opt/keycloak/certs/key.pem
      KC_HTTPS_PORT: "8443"
      # X.509 Client Certificate Authentication (request mode - allows but not requires)
      KC_HTTPS_CLIENT_AUTH: request
      # Trust mkcert root CA for federation with spoke Keycloaks
      KC_TRUSTSTORE_PATHS: /opt/keycloak/certs/mkcert-rootCA.pem
      KC_LOG_LEVEL: "debug,org.keycloak.events:debug,org.keycloak.services:debug,org.keycloak.authentication:debug"
      KC_METRICS_ENABLED: "true"
      KC_HEALTH_ENABLED: "true"
      KC_FEATURES: scripts
      KC_ADMIN: admin
      KC_ADMIN_PASSWORD: ${KC_ADMIN_PASSWORD}
      # Initial admin user creation (Keycloak 26.5.2)
      KC_BOOTSTRAP_ADMIN_USERNAME: admin
      KC_BOOTSTRAP_ADMIN_PASSWORD: ${KC_ADMIN_PASSWORD}
      # Bootstrap realm for Terraform provider authentication
      SKIP_REALM_IMPORT: "true"  # Terraform SSOT: Start empty, configure via Terraform
      # Legacy realm import variables (kept for compatibility, but not used)
      KEYCLOAK_CLIENT_SECRET: ${KEYCLOAK_CLIENT_SECRET:?KEYCLOAK_CLIENT_SECRET required}
      APP_URL: ${NEXT_PUBLIC_BASE_URL:-https://localhost:3000}
      API_URL: ${NEXT_PUBLIC_API_URL:-https://localhost:4000}
      USA_IDP_URL: ${USA_IDP_URL:-https://keycloak:8443}
      USA_IDP_CLIENT_SECRET: ${USA_IDP_CLIENT_SECRET:-default-usa-idp-secret}
      ADMIN_PASSWORD: ${ADMIN_PASSWORD:-DiveAdminSecure2025!}
      TEST_USER_PASSWORD: ${TEST_USER_PASSWORD:-DiveTestSecure2025!}
      INSTANCE_CODE: ${INSTANCE:-USA}
      # OpenTelemetry integration (Keycloak 26.5.2)
      OTEL_TRACES_EXPORTER: "otlp"
      OTEL_METRICS_EXPORTER: "otlp"
      OTEL_EXPORTER_OTLP_ENDPOINT: "http://otel-collector:4317"
      OTEL_SERVICE_NAME: "keycloak-${INSTANCE_CODE:-usa}"
    ports:
      - "127.0.0.1:${KEYCLOAK_HTTP_PORT:-8080}:8080"
      - "127.0.0.1:${KEYCLOAK_HTTPS_PORT:-8443}:8443"
      - "127.0.0.1:${KEYCLOAK_MGMT_PORT:-9000}:9000"
    volumes:
      - ./instances/hub/certs:/opt/keycloak/certs:ro
      - ./keycloak/themes:/opt/keycloak/themes:ro
      # ADDED (Dec 2025): Mount scripts to pick up local fixes without rebuilding image
      - ./keycloak/scripts:/opt/keycloak/scripts:ro
    networks:
      - hub-internal
      - dive-shared  # Required for federation (Keycloak needs to reach spoke Keycloaks)
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -ksf https://localhost:8443/realms/master >/dev/null"]
      interval: 5s         # Check every 5 seconds (was 10s)
      timeout: 8s
      retries: 10          # Allow 10 failures (was 10, unchanged)
      start_period: 45s    # Start checking after 45s (was 90s)

  # ---------------------------------------------------------------------------
  # OPA - Policy Decision Point (Hub loads bundle directly - INDUSTRY STANDARD)
  # ---------------------------------------------------------------------------
  # Architecture Note:
  #   Hub OPA loads policies from local bundle (standard OPA pattern)
  #   Spokes use OPAL clients to receive updates from Hub OPAL Server
  #   Hub does NOT need OPAL client (would be antipattern per OPAL docs)
  #
  # Data Loading:
  #   - Policies: Static bundle from /policies directory
  #   - Base Data: Minimal fallbacks in bundle (for cold start)
  #   - Dynamic Data: Backend API queries MongoDB directly (no OPAL needed)
  #
  # Why Hub Doesn't Need OPAL Client:
  #   1. Hub IS the source of truth (MongoDB)
  #   2. Hub backend queries MongoDB directly
  #   3. Only spokes need OPAL to receive updates from Hub
  #   4. OPAL Server + Client on same instance = antipattern (OPAL Discussion #390)
  # ---------------------------------------------------------------------------
  opa:
    labels:
      dive.service.class: "core"
      dive.service.description: "Open Policy Agent for ABAC authorization decisions"
    image: openpolicyagent/opa:1.12.3
    container_name: ${COMPOSE_PROJECT_NAME}-opa
    restart: unless-stopped
    command:
      - run
      - --server
      - --addr=0.0.0.0:8181
      - --tls-cert-file=/certs/certificate.pem
      - --tls-private-key-file=/certs/key.pem
      - --set=decision_logs.console=true
      - --set=data_api_enabled=true
      - --set=policies_api_enabled=true
      - --log-level=debug
      - /policies
    ports:
      - "127.0.0.1:${OPA_PORT:-8181}:8181"
      - "127.0.0.1:${OPA_METRICS_PORT:-8182}:8182"
    volumes:
      - ./policies:/policies:ro
      - ./instances/hub/certs:/certs:ro
    networks:
      - hub-internal
    healthcheck:
      test: ["CMD", "/opa", "version"]
      interval: 3s
      timeout: 3s
      retries: 5
      start_period: 5s

  # ---------------------------------------------------------------------------
  # Backend API - serves policy data for OPAL and Hub admin APIs
  # ---------------------------------------------------------------------------
  backend:
    labels:
      dive.service.class: "core"
      dive.service.description: "Express.js API with PEP for authorization enforcement"
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: ${COMPOSE_PROJECT_NAME}-backend
    restart: unless-stopped
    env_file:
      - .env.hub
    environment:
      NODE_ENV: development  # Development mode for better debugging
      LOG_LEVEL: warn  # Reduce log verbosity (info/warn/error)
      PORT: "4000"
      # Heap limit only in NODE_OPTIONS (report flags must be in command - Node disallows them in NODE_OPTIONS)
      NODE_OPTIONS: "--max-old-space-size=4096"
      INSTANCE_CODE: USA
      INSTANCE_NAME: "United States"
      INSTANCE_REALM: USA
      MONGODB_URL: mongodb://admin:${MONGO_PASSWORD}@mongodb:27017?authSource=admin&directConnection=true
      MONGODB_DATABASE: dive-v3-hub
      OPA_URL: https://opa:8181
      KEYCLOAK_URL: https://keycloak:8443
      KEYCLOAK_REALM: dive-v3-broker-usa
      KEYCLOAK_CLIENT_ID: ${KEYCLOAK_CLIENT_ID:-dive-v3-broker-usa}
      KEYCLOAK_CLIENT_SECRET: ${KEYCLOAK_CLIENT_SECRET}
      KEYCLOAK_JWKS_URI: https://keycloak:8443/realms/dive-v3-broker-usa/protocol/openid-connect/certs
      KEYCLOAK_ADMIN_USERNAME: admin
      KC_ADMIN_PASSWORD: ${KC_ADMIN_PASSWORD}
      # Redis configuration
      REDIS_PASSWORD: ${REDIS_PASSWORD_USA}
      REDIS_URL: redis://:${REDIS_PASSWORD_USA}@redis:6379
      # Shared blacklist Redis
      # Phase 2: Token Blacklist (local development uses dive-hub-redis)
      # For production, use shared-token-store for cross-instance revocation
      BLACKLIST_REDIS_URL: redis://:${REDIS_PASSWORD_BLACKLIST}@dive-hub-redis:6379
      # NOTE: Spoke Keycloak admin passwords come from spoke registration, NOT environment
      # The spoke provides its password during registration, stored in MongoDB
      NEXT_PUBLIC_BASE_URL: https://localhost:3000
      # SECURITY: Trust mkcert CA instead of disabling TLS verification
      NODE_EXTRA_CA_CERTS: /app/certs/ca/rootCA.pem
      OPAL_SERVER_URL: https://opal-server:7002
      OPAL_DATA_TOPICS: policy_data
      # OPAL JWT Auth: Backend obtains its own JWT via opalTokenService (see opal-client.ts)
      # The OPAL_AUTH_MASTER_TOKEN is used internally by opalTokenService to request JWTs
      OPAL_AUTH_MASTER_TOKEN: ${OPAL_AUTH_MASTER_TOKEN}
      # Docker exec configuration for running OPA tests
      OPA_CONTAINER: ${COMPOSE_PROJECT_NAME}-opa
      OPA_POLICIES_PATH: /policies
      # KAS URL for key requests (use container name since KAS is manually started)
      KAS_URL: https://kas:8080
      # Enable polling for file watching in Docker (fixes hot-reload)
      CHOKIDAR_USEPOLLING: "true"
      CHOKIDAR_INTERVAL: "1000"
    # Full stack/heap diagnostic on fatal errors (OOM) - report written to ./backend/logs
    # On-demand report: docker kill -s SIGUSR2 dive-hub-backend
    command:
      - node
      - --report-on-fatalerror
      - --report-uncaught-exception
      - --report-on-signal
      - --report-signal=SIGUSR2
      - --report-directory=/app/logs
      - --report-filename=node-report.%e.%p.%h.%t.json
      - node_modules/.bin/tsx
      - watch
      - src/https-server.ts
    ports:
      - "127.0.0.1:${BACKEND_PORT:-4000}:4000"
    volumes:
      - ./backend/src:/app/src
      - ./backend/package.json:/app/package.json:ro
      - ./backend/tsconfig.json:/app/tsconfig.json:ro
      - ./instances/hub/certs:/opt/keycloak/certs:ro
      - ./policies:/app/policies:ro
      - ./backend/certs:/app/certs:ro
      - ./certs/mkcert:/app/certs/ca:ro  # mkcert root CA for TLS trust
      - ./examples/examples:/app/examples/examples:ro  # Multi-format seeding templates
      - ./NATO_Security_Policy.xml:/app/NATO_Security_Policy.xml:ro
      - ./backend/logs:/app/logs
      - ./config:/app/config:ro
      # Docker socket for running OPA tests via docker exec
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - hub-internal
      - dive-shared  # Access to shared-blacklist-redis + federation
    depends_on:
      keycloak:
        condition: service_healthy
      mongodb:
        condition: service_healthy
      redis:
        condition: service_healthy
      # redis-blacklist: REMOVED - now in shared stack (shared-blacklist-redis)
      # Shared stack must be running before Hub deployment
      opa:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget --no-check-certificate -qO- https://localhost:4000/health || exit 1"]
      interval: 5s
      timeout: 10s
      retries: 10
      start_period: 45s

  # ---------------------------------------------------------------------------
  # AuthzForce - XACML Policy Decision Point (EXCLUDED - See ADR-001)
  # ---------------------------------------------------------------------------
  # STATUS: Disabled via docker-compose profile (not started by default)
  # REASON: Tomcat context startup failure causes 90s timeout on every deployment
  # IMPACT: Zero functional loss - OPA provides all authorization (XACML not required)
  # RE-ENABLE: docker compose --profile xacml up -d authzforce
  #
  # Technical Details:
  #   - Tomcat starts but WAR deployment fails ("listeners failed to start")
  #   - Root cause unclear, investigation not justified (OPTIONAL service, no clients)
  #   - Excluding saves 90s deployment time (59% reduction: 153s â†’ 63s)
  #
  # Alternative XACML Support:
  #   - Option A: Re-enable this service if XACML requirement emerges
  #   - Option B: Implement OPA XACML compatibility layer (Rego policies)
  #   - Option C: Evaluate alternative XACML PDPs (AT&T, WSO2 Balana)
  #
  # Platform Note (ARM64 hosts):
  #   AuthzForce image is linux/amd64 only. On ARM64 hosts (Apple Silicon, ARM servers),
  #   Docker uses QEMU emulation which adds ~20% performance overhead but works reliably.
  #   This is acceptable for dev/test environments. For production ARM64, consider:
  #   - Building a native ARM64 image from source, OR
  #   - Using an alternative XACML PDP with ARM64 support
  #
  # See: docs/ADR/ADR-001-AUTHZFORCE-EXCLUSION.md
  # ---------------------------------------------------------------------------
  authzforce:
    image: authzforce/server:12.0.1
    platform: linux/amd64  # Explicit for ARM64 hosts (runs via QEMU emulation)
    container_name: ${COMPOSE_PROJECT_NAME}-authzforce
    restart: unless-stopped
    # EXCLUDED from default deployment (ADR-001: Tomcat context failure, 90s timeout)
    # Re-enable: docker compose --profile xacml up
    profiles: ["xacml"]
    # NOTE: Exposing port for debugging only. In production, remove this.
    ports:
      - "127.0.0.1:${AUTHZFORCE_PORT:-8282}:8080"
    volumes:
      # Named volumes to prevent anonymous volumes from base image
      - authzforce_server:/opt/authzforce-ce-server
      - authzforce_tomcat_conf:/usr/local/tomcat/conf
      # Application-specific mounts (override subdirs of authzforce_server)
      - ./authzforce/conf:/opt/authzforce-ce-server/conf:ro
      - authzforce_data:/opt/authzforce-ce-server/data
      - ./policies/uploads:/policies:ro
    environment:
      JAVA_OPTS: -Xms256m -Xmx512m
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/authzforce-ce/domains"]
      interval: 10s
      timeout: 8s
      retries: 8
      start_period: 45s
    networks:
      - hub-internal

  # ---------------------------------------------------------------------------
  # OpenTelemetry Collector - Metrics and Tracing
  # ---------------------------------------------------------------------------
  otel-collector:
    labels:
      dive.service.class: "optional"
      dive.service.description: "OpenTelemetry collector for traces/metrics (optional)"
    image: otel/opentelemetry-collector:latest
    container_name: ${COMPOSE_PROJECT_NAME}-otel-collector
    restart: unless-stopped
    command: ["--config=/etc/otel-collector-config.yaml"]
    ports:
      - "127.0.0.1:4317:4317"  # OTLP gRPC
      - "127.0.0.1:4318:4318"  # OTLP HTTP
      - "127.0.0.1:8889:8889"  # Prometheus exporter (for Grafana)
      - "127.0.0.1:13133:13133"  # Health check endpoint
    volumes:
      - ./monitoring/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    networks:
      - hub-internal
    # Note: No Docker health check for distroless image
    # The collector's health_check extension runs on port 13133 for external monitoring
    # Classification: OPTIONAL (observability enhancement, not core functionality)
    # Relies on restart policy for failure recovery
    depends_on:
      - keycloak
      - backend

  # ---------------------------------------------------------------------------
  # KAS - Key Access Service (Stretch Goal)
  # ---------------------------------------------------------------------------
  kas:
    labels:
      dive.service.class: "stretch"
      dive.service.description: "Key Access Service for TDF encrypted resources (stretch goal)"
    build:
      context: ./kas
      dockerfile: Dockerfile
    container_name: ${COMPOSE_PROJECT_NAME}-kas
    restart: unless-stopped
    env_file:
      - .env.hub
    healthcheck:
      # FIX: Use curl instead of wget (curl is installed in KAS Dockerfile, wget is not)
      test: ["CMD-SHELL", "curl -kfs https://localhost:8080/health || exit 1"]
      interval: 5s
      timeout: 8s
      retries: 8
      start_period: 20s
    environment:
      NODE_ENV: development
      KAS_PORT: 8080
      HTTPS_ENABLED: "true"
      CERT_PATH: /opt/app/certs
      KEY_FILE: key.pem
      CERT_FILE: certificate.pem
      BACKEND_URL: https://backend:4000
      # SECURITY: Trust mkcert CA instead of disabling TLS verification
      NODE_EXTRA_CA_CERTS: /app/certs/ca/rootCA.pem
      OPA_URL: https://opa:8181
      MONGODB_URL: mongodb://admin:${MONGO_PASSWORD}@mongodb:27017?authSource=admin
      MONGODB_DATABASE: dive-v3-hub
      LOG_LEVEL: debug
      KEYCLOAK_URL: https://keycloak:8443
      KEYCLOAK_REALM: dive-v3-broker-usa
      KEYCLOAK_CLIENT_ID: ${KEYCLOAK_CLIENT_ID:-dive-v3-broker-usa}
    ports:
      - "127.0.0.1:8085:8080"
    depends_on:
      - opa
      - mongodb
    networks:
      - hub-internal
    volumes:
      - ./kas/certs:/opt/app/certs:ro
      - ./certs/mkcert:/app/certs/ca:ro  # mkcert root CA for TLS trust
      - ./kas/logs:/app/logs
      - ./config:/app/config:ro
    # Use built version instead of dev mode
    command: node dist/server.js

  # ---------------------------------------------------------------------------
  # Frontend - Next.js Application
  # ---------------------------------------------------------------------------
  frontend:
    labels:
      dive.service.class: "core"
      dive.service.description: "Next.js React application with NextAuth.js"
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    container_name: ${COMPOSE_PROJECT_NAME}-frontend
    restart: unless-stopped
    env_file:
      - .env.hub
    healthcheck:
      test: ["CMD-SHELL", "curl -ksf https://localhost:3000/ >/dev/null || exit 1"]
      interval: 5s         # Check every 5 seconds (was 10s)
      timeout: 8s
      retries: 10          # Allow 10 failures (was 12)
      start_period: 30s    # Start checking after 30s (was 90s)
    environment:
      NODE_ENV: development
      # Instance identification for frontend theme/branding
      NEXT_PUBLIC_INSTANCE: USA
      NEXT_PUBLIC_INSTANCE_NAME: "United States"
      NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL:-https://localhost:4000}
      NEXT_PUBLIC_BACKEND_URL: ${NEXT_PUBLIC_BACKEND_URL:-https://localhost:4000}
      NEXT_PUBLIC_BASE_URL: ${NEXT_PUBLIC_BASE_URL:-https://localhost:3000}
      BACKEND_URL: https://backend:4000
      KEYCLOAK_BASE_URL: https://keycloak:8443
      KEYCLOAK_URL: ${KEYCLOAK_URL:-https://localhost:8443}
      KEYCLOAK_REALM: ${KEYCLOAK_REALM:-dive-v3-broker-usa}
      NEXT_PUBLIC_KEYCLOAK_URL: ${NEXT_PUBLIC_KEYCLOAK_URL:-https://localhost:8443}
      NEXT_PUBLIC_KEYCLOAK_REALM: ${NEXT_PUBLIC_KEYCLOAK_REALM:-dive-v3-broker-usa}
      KEYCLOAK_ISSUER: ${KEYCLOAK_ISSUER:-https://localhost:8443/realms/dive-v3-broker-usa}
      NEXTAUTH_URL: ${NEXTAUTH_URL:-https://localhost:3000}
      NEXT_PUBLIC_EXTERNAL_DOMAINS: "${NEXT_PUBLIC_EXTERNAL_DOMAINS:-https://localhost:3000,https://localhost:4000,https://localhost:8443}"
      AUTH_SECRET: ${AUTH_SECRET:-default-auth-secret-change-me}
      NEXTAUTH_SECRET: ${AUTH_SECRET:-default-auth-secret-change-me}
      KEYCLOAK_CLIENT_ID: ${KEYCLOAK_CLIENT_ID:-dive-v3-broker-usa}
      KEYCLOAK_CLIENT_SECRET: ${KEYCLOAK_CLIENT_SECRET}
      AUTH_KEYCLOAK_ID: ${KEYCLOAK_CLIENT_ID:-dive-v3-broker-usa}
      AUTH_KEYCLOAK_SECRET: ${KEYCLOAK_CLIENT_SECRET}
      AUTH_KEYCLOAK_ISSUER: ${AUTH_KEYCLOAK_ISSUER:-https://localhost:8443/realms/dive-v3-broker-usa}
      AUTH_TRUST_HOST: "true"
      # TLS Certificate Handling - Proper mkcert Integration (2026 Best Practice)
      # Makes Node.js fetch() API use system CA trust store (mkcert CA installed via entrypoint)
      # See: docs/TLS_BEST_PRACTICES_2026.md
      NODE_OPTIONS: "--use-openssl-ca"
      NODE_EXTRA_CA_CERTS: /app/certs/ca/rootCA.pem  # Legacy https module support
      DATABASE_URL: postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/dive_v3_app
    ports:
      - "127.0.0.1:${FRONTEND_PORT:-3000}:3000"
    depends_on:
      keycloak:
        condition: service_healthy
      backend:
        condition: service_healthy
    networks:
      - hub-internal
    extra_hosts:
      - "localhost:host-gateway"
    volumes:
      # DEVELOPMENT: Enable hot reload with source mounts
      # WARNING: These volume mounts will overlay the production image contents
      # For production deployment, create docker-compose.hub.prod.yml without these mounts
      - ./frontend:/app  # Source code for hot reload
      - ./instances/hub/certs:/opt/keycloak/certs:ro
      - ./frontend/certs:/opt/app/certs:ro
      - ./certs/mkcert:/app/certs/ca:ro  # mkcert root CA for TLS trust (entrypoint expects /app/certs/ca/rootCA.pem)
      # Named volumes for persistence (development hot reload)
      - frontend_node_modules:/app/node_modules
      - frontend_next:/app/.next

  # ---------------------------------------------------------------------------
  # OPAL Server - Policy distribution hub
  # ---------------------------------------------------------------------------
  opal-server:
    labels:
      dive.service.class: "stretch"
      dive.service.description: "OPAL server for real-time policy distribution (stretch goal)"
    build:
      context: ./docker
      dockerfile: opal-server.Dockerfile
    container_name: ${COMPOSE_PROJECT_NAME}-opal-server
    restart: unless-stopped
    stop_grace_period: 30s
    env_file:
      - .env.hub
    environment:
      # Use Redis as broadcast backend for pub/sub
      OPAL_BROADCAST_URI: redis://:${REDIS_PASSWORD_USA}@redis:6379
      # Local policy distribution with filesystem watching (not Git)
      # CRITICAL: Use master-branch tracking to match actual Git branch
      OPAL_POLICY_REPO_URL: file:///policies
      OPAL_POLICY_REPO_MAIN_BRANCH: master
      OPAL_REPO_WATCHER_ENABLED: "true"
      OPAL_POLICY_REPO_POLLING_INTERVAL: 5
      # Force policy reload on every poll (for development)
      OPAL_POLICY_REPO_CLONE_PATH: /tmp/opal_repo
      OPAL_POLICY_REPO_REUSE_CLONE_PATH: "false"
      # Policy paths to include (all .rego files in /policies)
      OPAL_POLICY_SOURCE_DIRS: .,base,org,tenant,entrypoints,compat,data
      # Multiple data topics for real-time policy data distribution (Phase 2-3)
      OPAL_DATA_TOPICS_DEFAULT: policy_data,trusted_issuers,federation_matrix,tenant_configs,coi_definitions,federation_constraints
      OPAL_INLINE_OPA_CONFIG: "false"
      OPAL_LOG_LEVEL: DEBUG
      OPAL_LOG_FORMAT: text
      UVICORN_PORT: 7002
      UVICORN_HOST: 0.0.0.0
      OPAL_SERVER_WORKERS: 1
      UVICORN_SSL_CERTFILE: /certs/certificate.pem
      UVICORN_SSL_KEYFILE: /certs/key.pem
      OPAL_STATISTICS_ENABLED: "true"
      OPAL_SERVER_CORS_ALLOWED_ORIGINS: '["http://localhost:3000","https://localhost:3000","http://localhost:7002"]'
      # Data source URLs with multiple topics (Phase 2: Real-Time Policy Data)
      # Each endpoint serves dynamic data from MongoDB (with file fallback)
      # - policy_data: Combined policy data (legacy, for compatibility)
      # - trusted_issuers: Dynamic trusted IdP issuers from MongoDB
      # - federation_matrix: Dynamic federation trust relationships
      # - tenant_configs: Per-nation tenant configurations
      OPAL_DATA_CONFIG_SOURCES: |
        {
          "config": {
            "entries": [
              {
                "url": "https://host.docker.internal:4000/api/opal/policy-data",
                "topics": ["policy_data"],
                "dst_path": "dive/federation",
                "config": {"headers": {"Accept": "application/json"}}
              },
              {
                "url": "https://host.docker.internal:4000/api/opal/trusted-issuers",
                "topics": ["trusted_issuers"],
                "dst_path": "trusted_issuers",
                "config": {"headers": {"Accept": "application/json"}}
              },
              {
                "url": "https://host.docker.internal:4000/api/opal/federation-matrix",
                "topics": ["federation_matrix"],
                "dst_path": "federation_matrix",
                "config": {"headers": {"Accept": "application/json"}}
              },
              {
                "url": "https://host.docker.internal:4000/api/opal/tenant-configs",
                "topics": ["tenant_configs"],
                "dst_path": "tenant_configs",
                "config": {"headers": {"Accept": "application/json"}}
              },
              {
                "url": "https://host.docker.internal:4000/api/opal/coi-definitions",
                "topics": ["coi_definitions"],
                "dst_path": "coi_definitions",
                "config": {"headers": {"Accept": "application/json"}}
              },
              {
                "url": "https://host.docker.internal:4000/api/opal/federation-constraints",
                "topics": ["federation_constraints"],
                "dst_path": "federation_constraints",
                "config": {"headers": {"Accept": "application/json"}},
                "description": "Phase 2: Tenant-controlled federation constraints (bilateral)"
              }
            ]
          }
        }
      # OPAL Authentication - JWT token verification for spoke clients
      OPAL_AUTH_MASTER_TOKEN: ${OPAL_AUTH_MASTER_TOKEN}
    ports:
      - "127.0.0.1:${OPAL_PORT:-7002}:7002"
    networks:
      - hub-internal
      - dive-shared  # Allow spoke OPAL clients to connect via federation network
    volumes:
      # Local policy bundle for distribution to spokes
      - ./policies:/policies:ro
      - ./opal-data-source:/opal-data-source:ro
      - ./instances/hub/certs:/certs:ro
      - ./certs/opal:/opal-keys:ro
    # Uses custom entrypoint from Dockerfile that loads auth keys
    healthcheck:
      test: ["CMD", "curl", "-k", "-f", "https://localhost:7002/healthcheck"]
      interval: 5s
      timeout: 8s
      retries: 10
      start_period: 30s
    depends_on:
      - opa
      - backend
