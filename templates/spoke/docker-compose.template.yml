# =============================================================================
# DIVE V3 Spoke Instance: {{INSTANCE_CODE_UPPER}} ({{INSTANCE_NAME}})
# =============================================================================
# Generated At: {{TIMESTAMP}}
# Template Hash: {{TEMPLATE_HASH}}
# Instance Code: {{INSTANCE_CODE_UPPER}}
# NOTE: This file is auto-generated from templates/spoke/docker-compose.template.yml
#       Do not edit directly - changes will be overwritten on next deployment
# =============================================================================
#
# BREAKING CHANGES in 2.5.0:
# - Unified network naming: 'internal' instead of 'dive-${code}-network'
# - Simplified volume declarations (no code prefix, relies on project name)
# - Keycloak 24.0.0 (from 23.0.0)
# - MongoDB 7.0 (from 6.0)
# - OPA 0.68.0 (from 0.60.0)
#
# FIXES in 2.5.1:
# - Explicitly mapped MongoDB /data/configdb to named volume (mongodb_config)
# - Explicitly mapped OPAL client cache to named volume (opal_cache)
# - Eliminates anonymous Docker volumes created by base images
#
# CRITICAL FIX in 2.6.0:
# - Added container_name directives to all 9 services
# - Prevents Docker Compose auto-naming with -1 suffix
# - Ensures SSOT compliance: dive-spoke-{code}-{service}
# - No more "dive-spoke-pol-keycloak-pol-1" (now "dive-spoke-pol-keycloak")
#
# VERSION ALIGNMENT in 2.7.0:
# - Fixed Redis version: redis:alpine → redis:7-alpine (matches Hub)
# - Fixed OPA version: openpolicyagent/opa:0.68.0 → openpolicyagent/opa:1.12.1 (latest stable)
# - Ensures Hub and Spoke use identical base image versions
# - See DOCKER_IMAGE_VERSION_SSOT.md for complete version matrix
#
# NETWORK FIX in 2.8.0:
# - Backend now on BOTH dive-internal AND dive-shared networks
# - CRITICAL: dive-shared required for Hub federation and OPAL policy sync
# - Fixes "getaddrinfo ENOTFOUND hub.dive25.com" errors
#
# KEYCLOAK 26.5.2 UPGRADE in 2.9.1:
# - Updated Keycloak to version 26.5.2 with latest security fixes
# - Replaced deprecated KEYCLOAK_ADMIN/KEYCLOAK_ADMIN_PASSWORD with KC_ADMIN/KC_ADMIN_PASSWORD
# - Ensures compatibility with latest Keycloak configuration standards
# =============================================================================
#
# This is a STANDALONE spoke stack - completely separate from Hub.
#
# Placeholders (auto-replaced by ./dive spoke init):
#   {{INSTANCE_CODE_UPPER}} - 3-letter uppercase code (e.g., NZL)
#   {{INSTANCE_CODE_LOWER}} - 3-letter lowercase code (e.g., nzl)
#   {{INSTANCE_NAME}}       - Human-readable name
#   {{IDP_HOSTNAME}}        - Keycloak hostname
#   {{API_URL}}             - Backend API URL
#   {{BASE_URL}}            - Frontend URL
#   {{IDP_URL}}             - Keycloak IdP URL
#   {{KEYCLOAK_HOST_PORT}}  - Keycloak HTTPS port (e.g., 18443)
#   {{BACKEND_HOST_PORT}}   - Backend port (e.g., 14000)
#   {{FRONTEND_HOST_PORT}}  - Frontend port (e.g., 13000)
#   {{OPA_HOST_PORT}}       - OPA port (e.g., 18181)
#   {{KAS_HOST_PORT}}       - KAS port (e.g., 18085)
#
# Usage:
#   cd instances/{{INSTANCE_CODE_LOWER}}
#   docker compose --env-file .env up -d
#
# =============================================================================

# Project name ensures complete isolation from Hub
name: dive-spoke-{{INSTANCE_CODE_LOWER}}

networks:
  dive-internal:
    driver: bridge
    labels:
      dive.network.type: "internal"
      dive.network.scope: "spoke"
      dive.network.instance: "{{INSTANCE_CODE_UPPER}}"
      dive.network.description: "Spoke {{INSTANCE_CODE_UPPER}} internal service communication"
  dive-shared:
    external: true

volumes:
  postgres_data:
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "postgres"
      dive.resource.instance: "{{INSTANCE_CODE_UPPER}}"
      dive.resource.purpose: "database"
  mongodb_data:
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "mongodb"
      dive.resource.instance: "{{INSTANCE_CODE_UPPER}}"
      dive.resource.purpose: "database"
  mongodb_config:  # MongoDB config directory (prevent anonymous volume)
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "mongodb"
      dive.resource.instance: "{{INSTANCE_CODE_UPPER}}"
      dive.resource.purpose: "config"
  redis_data:
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "redis"
      dive.resource.instance: "{{INSTANCE_CODE_UPPER}}"
      dive.resource.purpose: "cache"
  opal_cache:
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "opal-client"
      dive.resource.instance: "{{INSTANCE_CODE_UPPER}}"
      dive.resource.purpose: "cache"
  opal_backup:     # OPAL client backup directory (prevent anonymous volume from base image)
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "opal-client"
      dive.resource.instance: "{{INSTANCE_CODE_UPPER}}"
      dive.resource.purpose: "backup"
  frontend_modules:
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "frontend"
      dive.resource.instance: "{{INSTANCE_CODE_UPPER}}"
      dive.resource.purpose: "node_modules"
  frontend_next:
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "frontend"
      dive.resource.instance: "{{INSTANCE_CODE_UPPER}}"
      dive.resource.purpose: "build_cache"
  backend_node_modules:
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "backend"
      dive.resource.instance: "{{INSTANCE_CODE_UPPER}}"
      dive.resource.purpose: "node_modules"
  backend_logs:
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "backend"
      dive.resource.instance: "{{INSTANCE_CODE_UPPER}}"
      dive.resource.purpose: "logs"
  kas_logs:
    labels:
      dive.resource.type: "volume"
      dive.resource.service: "kas"
      dive.resource.instance: "{{INSTANCE_CODE_UPPER}}"
      dive.resource.purpose: "logs"

services:
  # ==========================================================================
  # DATABASE SERVICES
  # ==========================================================================
  # NOTE: All secrets use instance-suffixed environment variables
  # Example: ${POSTGRES_PASSWORD_NZL} (not ${POSTGRES_PASSWORD})
  # This ensures each spoke has isolated, unique credentials
  # Secret management: scripts/dive-modules/spoke/pipeline/spoke-secrets.sh
  # ==========================================================================

  postgres-{{INSTANCE_CODE_LOWER}}:
    labels:
      dive.service.class: "core"
      dive.service.description: "PostgreSQL database for Keycloak user/realm storage"
    image: postgres:18.1-alpine3.23
    container_name: dive-spoke-{{INSTANCE_CODE_LOWER}}-postgres
    environment:
      POSTGRES_USER: keycloak
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD_{{INSTANCE_CODE_UPPER}}}
      POSTGRES_DB: keycloak
    volumes:
      - postgres_data:/var/lib/postgresql  # Mount parent dir to prevent anonymous volume
    networks:
      - dive-internal
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U keycloak"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  mongodb-{{INSTANCE_CODE_LOWER}}:
    labels:
      dive.service.class: "core"
      dive.service.description: "MongoDB database for resource metadata and audit logs"
    image: mongo:8.0.17
    container_name: dive-spoke-{{INSTANCE_CODE_LOWER}}-mongodb
    # Use official docker-entrypoint with custom command for replica set + keyFile
    # KeyFile permissions fixed via init script before MongoDB starts
    command: >
      bash -c "
        cp /data/keyfile/mongo-keyfile /tmp/mongo-keyfile &&
        chmod 600 /tmp/mongo-keyfile &&
        chown 999:999 /tmp/mongo-keyfile &&
        exec docker-entrypoint.sh mongod --replSet rs0 --keyFile /tmp/mongo-keyfile
      "
    environment:
      MONGO_INITDB_DATABASE: dive-v3-{{INSTANCE_CODE_LOWER}}
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_PASSWORD_{{INSTANCE_CODE_UPPER}}}
    volumes:
      - mongodb_data:/data/db
      - mongodb_config:/data/configdb  # Prevent anonymous volume creation
      # Mount replica set keyFile (generated during spoke deployment)
      - ./mongo-keyfile:/data/keyfile/mongo-keyfile:ro
      # NOTE: Replica set initialization happens POST-START via deployment script
      # docker-entrypoint-initdb.d/ runs BEFORE --replSet is applied, so it cannot work
      # See: scripts/init-mongo-replica-set-post-start.sh (called by spoke deployment pipeline)
    networks:
      - dive-internal
    healthcheck:
      # CRITICAL FIX (2026-02-06): Initialize replica set AND verify PRIMARY status
      # This ensures MongoDB is TRULY READY before dependent services (KAS, backend) start
      #
      # Root Cause: Docker healthcheck must test READINESS, not just connectivity
      # - Phase 1: Check if replica set exists, initialize if not
      # - Phase 2: Verify node is PRIMARY (not SECONDARY/RECOVERING/STARTUP)
      # - Phase 3: Only mark healthy when PRIMARY is confirmed
      #
      # This prevents race conditions where:
      # - KAS starts before MongoDB replica set is initialized
      # - Backend connects before PRIMARY election completes
      # - Services get "node is not in primary or recovering state" errors
      test: |
        mongosh admin -u admin -p ${MONGO_PASSWORD_{{INSTANCE_CODE_UPPER}}} --quiet --eval "
          try {
            const status = rs.status();

            // Check if this node is PRIMARY
            if (status.ok === 1 && status.myState === 1) {
              // PRIMARY confirmed - truly ready!
              quit(0);
            }

            // Replica set exists but not PRIMARY yet
            // Log current state for debugging
            if (status.myState === 2) {
              print('MongoDB in SECONDARY state - waiting for PRIMARY election');
            } else if (status.myState === 5) {
              print('MongoDB in STARTUP2 state - initializing replica set');
            } else {
              print('MongoDB state: ' + status.myState + ' - waiting for PRIMARY');
            }
            quit(1);

          } catch (e) {
            // Replica set not configured yet - initialize it
            if (e.message.includes('no replset config') || e.message.includes('not running with --replSet')) {
              print('[MongoDB] Replica set not initialized - initializing now...');

              const initResult = rs.initiate({
                _id: 'rs0',
                members: [{
                  _id: 0,
                  host: 'localhost:27017',
                  priority: 1
                }]
              });

              if (initResult.ok === 1) {
                print('[MongoDB] Replica set initialization successful');
                // Give it time to elect PRIMARY (typically 5-10 seconds)
                sleep(8000);

                // Verify PRIMARY status after initialization
                try {
                  const statusAfterInit = rs.status();
                  if (statusAfterInit.ok === 1 && statusAfterInit.myState === 1) {
                    print('[MongoDB] PRIMARY elected - ready for connections');
                    quit(0);
                  } else {
                    print('[MongoDB] Replica set initializing, state: ' + statusAfterInit.myState);
                    quit(1);
                  }
                } catch (verifyError) {
                  print('[MongoDB] Waiting for replica set to stabilize...');
                  quit(1);
                }
              } else {
                print('[MongoDB] Replica set initialization failed: ' + JSON.stringify(initResult));
                quit(1);
              }
            } else {
              print('[MongoDB] Error checking replica set status: ' + e.message);
              quit(1);
            }
          }
        "
      interval: 5s
      timeout: 10s
      retries: 24
      start_period: 40s
    restart: unless-stopped

  redis-{{INSTANCE_CODE_LOWER}}:
    labels:
      dive.service.class: "core"
      dive.service.description: "Redis cache for session state and policy decisions"
    image: redis:7-alpine
    container_name: dive-spoke-{{INSTANCE_CODE_LOWER}}-redis
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD_{{INSTANCE_CODE_UPPER}}:-}
      --appendonly yes
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    networks:
      - dive-internal
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD_{{INSTANCE_CODE_UPPER}}:-}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # ==========================================================================
  # IDENTITY & ACCESS MANAGEMENT
  # ==========================================================================

  keycloak-{{INSTANCE_CODE_LOWER}}:
    labels:
      dive.service.class: "core"
      dive.service.description: "Keycloak IdP for federation and local authentication"
    build:
      context: ../../keycloak
      dockerfile: Dockerfile
    container_name: dive-spoke-{{INSTANCE_CODE_LOWER}}-keycloak
    entrypoint: ["/bin/bash", "/opt/keycloak/scripts/import-realm.sh"]
    command: ["start-dev", "--spi-login-protocol-openid-connect-suppress-logout-confirmation-screen=true", "--features=scripts"]
    environment:
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://postgres-{{INSTANCE_CODE_LOWER}}:5432/keycloak
      KC_DB_USERNAME: keycloak
      KC_DB_PASSWORD: ${POSTGRES_PASSWORD_{{INSTANCE_CODE_UPPER}}}
      KC_BOOTSTRAP_ADMIN_USERNAME: admin
      KC_BOOTSTRAP_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD_{{INSTANCE_CODE_UPPER}}}
      KC_HOSTNAME: {{IDP_HOSTNAME}}
      KC_HOSTNAME_STRICT: "false"
      KC_PROXY_HEADERS: xforwarded
      KC_HTTP_ENABLED: "true"
      KC_HTTPS_CERTIFICATE_FILE: /opt/keycloak/certs/certificate.pem
      KC_HTTPS_CERTIFICATE_KEY_FILE: /opt/keycloak/certs/key.pem
      KC_HTTPS_PORT: "8443"
      # X.509 Client Certificate Authentication (request mode - allows but not requires)
      KC_HTTPS_CLIENT_AUTH: request
      KC_LOG_LEVEL: info
      KC_FEATURES: scripts
      KC_HEALTH_ENABLED: "true"
      KC_METRICS_ENABLED: "true"
      # ==========================================================================
      # TRUST STORE FOR FEDERATION (Keycloak 26+ native approach)
      # ==========================================================================
      # Keycloak needs to trust self-signed certificates from other Keycloak
      # instances during federation. This is required for server-to-server calls
      # (tokenUrl, userInfoUrl, jwksUrl). Without this:
      #   "PKIX path building failed: unable to find valid certification path"
      #
      # The rootCA.pem is synced during deployment via:
      #   spoke_init_prepare_certificates() in phase-initialization.sh
      # ==========================================================================
      KC_TRUSTSTORE_PATHS: /opt/keycloak/certs/ca/rootCA.pem
      # Terraform SSOT: Skip JSON import, Keycloak starts empty
      SKIP_REALM_IMPORT: "true"
      # Legacy realm import variables (kept for compatibility, but not used)
      KEYCLOAK_CLIENT_SECRET: ${KEYCLOAK_CLIENT_SECRET_{{INSTANCE_CODE_UPPER}}}
      APP_URL: {{BASE_URL}}
      API_URL: {{API_URL}}
      USA_IDP_URL: https://keycloak-{{INSTANCE_CODE_LOWER}}:8443
      USA_IDP_CLIENT_SECRET: ${KEYCLOAK_CLIENT_SECRET_USA}
      ADMIN_PASSWORD: ${ADMIN_PASSWORD:-DiveAdminSecure2025!}
      TEST_USER_PASSWORD: ${TEST_USER_PASSWORD:-TestUser2025!Pilot}  # SSOT: Same as seed-spoke-users.sh
      INSTANCE_CODE: {{INSTANCE_CODE_UPPER}}
    ports:
      - "{{KEYCLOAK_HOST_PORT}}:8443"
      - "{{KEYCLOAK_HTTP_PORT}}:8080"
    volumes:
      - ./certs:/opt/keycloak/certs:ro
      - ../../keycloak/themes:/opt/keycloak/themes:ro
      - ../../keycloak/realms:/opt/keycloak/realm-templates:ro
    depends_on:
      postgres-{{INSTANCE_CODE_LOWER}}:
        condition: service_healthy
    networks:
      dive-internal:
      dive-shared:
        aliases:
          - keycloak-{{INSTANCE_CODE_LOWER}}  # CRITICAL: Network alias for federation JWKS discovery
    healthcheck:
      # ENHANCED: Check spoke realm (application ready) with fallback to master (Keycloak operational)
      # Primary: Verifies Terraform configuration applied (spoke realm exists)
      # Fallback: Verifies Keycloak is operational (during Terraform apply phase)
      test: ["CMD-SHELL", "curl -ksf https://localhost:8443/realms/dive-v3-broker-{{INSTANCE_CODE_LOWER}} >/dev/null 2>&1 || curl -ksf https://localhost:8443/realms/master >/dev/null 2>&1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    restart: unless-stopped

  # ==========================================================================
  # POLICY ENGINE
  # ==========================================================================

  opa-{{INSTANCE_CODE_LOWER}}:
    labels:
      dive.service.class: "core"
      dive.service.description: "Open Policy Agent for ABAC authorization decisions"
    image: openpolicyagent/opa:1.12.3
    container_name: dive-spoke-{{INSTANCE_CODE_LOWER}}-opa
    platform: linux/amd64
    command: run --server --addr :8181 --set=decision_logs.console=true --set=data_api_enabled=true --set=policies_api_enabled=true --tls-cert-file=/certs/certificate.pem --tls-private-key-file=/certs/key.pem
    ports:
      - "{{OPA_HOST_PORT}}:8181"
    volumes:
      # REMOVED: ../../policies:/policies:ro
      # Policies now loaded exclusively via OPAL (proper GitOps)
      - ./cache/policies:/var/opa/cache
      - ./certs:/certs:ro
    networks:
      - dive-internal
    healthcheck:
      # FIXED: Use CMD (not CMD-SHELL) as OPA minimal image has no /bin/sh
      # Test basic OPA evaluation (policies loaded from mounted volumes)
      test: ["CMD", "/opa", "eval", "--format", "raw", "true"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  opal-client-{{INSTANCE_CODE_LOWER}}:
    labels:
      dive.service.class: "optional"
      dive.service.description: "OPAL client for real-time policy sync with hub (optional)"
    build:
      context: ../../docker
      dockerfile: opal-client.Dockerfile
    container_name: dive-spoke-{{INSTANCE_CODE_LOWER}}-opal-client
    entrypoint: ["/bin/bash", "-c"]
    command: ["/usr/local/bin/opal-entrypoint.sh"]
    environment:
      # Hub OPAL Server URL - from environment variable (required)
      OPAL_SERVER_URL: ${HUB_OPAL_URL:?HUB_OPAL_URL required - set in .env}
      # Authentication - token and public key from Hub
      OPAL_CLIENT_TOKEN: ${SPOKE_OPAL_TOKEN:-}
      OPAL_AUTH_PUBLIC_KEY: ${OPAL_AUTH_PUBLIC_KEY:-}
      # Policy subscription directories - CRITICAL FIX for duplicate paths issue
      # Request only specific directories from Hub (NOT "." which requests entire repo)
      # This prevents OPAL from including files with "policies/" prefix duplicates
      # Delimiter is colon (:) for OPAL client config
      OPAL_POLICY_SUBSCRIPTION_DIRS: base:org:tenant:entrypoints:compat
      # Use external OPA (standalone container) - OPAL pushes policies to it
      # This avoids inline OPA chicken-and-egg health check issues
      OPAL_INLINE_OPA_ENABLED: "false"
      OPAL_POLICY_STORE_URL: https://opa-{{INSTANCE_CODE_LOWER}}:8181
      # SSOT: Use INSTANCE_CODE for subscription, not spokeId (Hub is SSOT for spokeId)
      OPAL_SUBSCRIPTION_ID: spoke-{{INSTANCE_CODE_LOWER}}
      # Data topics to subscribe to (must match Hub topics)
      # CRITICAL: Subscribe to ALL data topics for dynamic trusted issuers, federation matrix, etc.
      OPAL_DATA_TOPICS: policy_data,trusted_issuers,federation_matrix,tenant_configs,federation_constraints
      OPAL_LOG_LEVEL: INFO
      # Enable data updater to receive dynamic data from Hub
      OPAL_DATA_UPDATER_ENABLED: "true"
      OPAL_KEEP_ALIVE_TIMEOUT: 60
      OPAL_RECONNECT_INTERVAL: 5
      OPAL_RECONNECT_MAX_INTERVAL: 300
      OPAL_POLICY_REFRESH_INTERVAL: 60
      # SSL Certificate Trust - CRITICAL for Hub OPAL server connection
      # CA certs are in /ca/ subdirectory, combined in entrypoint
      SSL_CERT_FILE: /var/opal/hub-certs/ca/rootCA.pem
      REQUESTS_CA_BUNDLE: /var/opal/hub-certs/ca/rootCA.pem
      WEBSOCKET_SSL_CERT: /var/opal/hub-certs/ca/rootCA.pem
    ports:
      - "{{OPAL_OPA_PORT}}:8181"  # Expose inline OPA for backend
    volumes:
      - opal_cache:/var/opal/cache
      - opal_backup:/opal/backup  # Prevent anonymous volume from base image
      - ./certs:/var/opal/certs:ro
      - ../../instances/hub/certs:/var/opal/hub-certs:ro  # Mount Hub CA for SSL trust
    networks:
      - dive-internal      # Internal spoke communication
      - dive-shared        # CRITICAL: Required for Hub OPAL server communication
    extra_hosts:
      - "localhost:host-gateway"
    healthcheck:
      # Check OPAL client API health (port 7000), not inline OPA
      test: ["CMD", "curl", "-f", "http://localhost:7000/healthcheck"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    # NO profiles restriction - ALWAYS start for policy sync with Hub
    # Uses external standalone OPA (opa-{{INSTANCE_CODE_LOWER}})

  # ==========================================================================
  # BACKEND API
  # ==========================================================================

  backend-{{INSTANCE_CODE_LOWER}}:
    labels:
      dive.service.class: "core"
      dive.service.description: "Express.js API with PEP for authorization enforcement"
    build:
      context: ../../backend
      dockerfile: Dockerfile.dev
    container_name: dive-spoke-{{INSTANCE_CODE_LOWER}}-backend
    init: true
    command: ["/bin/sh","-c","mkdir -p /app/certs/crl && npm install && npm run dev"]
    environment:
      NODE_ENV: development
      # SECURITY: Trust mkcert CA instead of disabling TLS verification
      NODE_EXTRA_CA_CERTS: /app/certs/ca/rootCA.pem
      PORT: "4000"
      # SSOT ARCHITECTURE (2026-01-22): Backend uses INSTANCE_CODE to query Hub for spokeId
      # No SPOKE_ID environment variable - Hub MongoDB is single source of truth
      INSTANCE_CODE: {{INSTANCE_CODE_UPPER}}
      INSTANCE_NAME: "{{INSTANCE_NAME}}"
      INSTANCE_REALM: {{INSTANCE_CODE_UPPER}}
      SPOKE_MODE: "true"
      IS_HUB: "false"
      # MongoDB
      MONGODB_URI: mongodb://admin:${MONGO_PASSWORD_{{INSTANCE_CODE_UPPER}}}@mongodb-{{INSTANCE_CODE_LOWER}}:27017/dive-v3-{{INSTANCE_CODE_LOWER}}?authSource=admin&directConnection=true
      MONGODB_URL: mongodb://admin:${MONGO_PASSWORD_{{INSTANCE_CODE_UPPER}}}@mongodb-{{INSTANCE_CODE_LOWER}}:27017/?authSource=admin&directConnection=true
      MONGODB_DATABASE: dive-v3-{{INSTANCE_CODE_LOWER}}
      # Local Redis for session cache (authenticated)
      REDIS_URL: redis://:${REDIS_PASSWORD_{{INSTANCE_CODE_UPPER}}:-}@redis-{{INSTANCE_CODE_LOWER}}:6379
      # ==========================================================================
      # SHARED BLACKLIST REDIS (ACP-240 Cross-Instance Token Revocation)
      # ==========================================================================
      # Connects to Hub's centralized blacklist Redis for federation-wide token
      # revocation. When a user logs out on any spoke, their tokens are revoked
      # across ALL federated instances (GAP-010 remediation).
      #
      # Requirements:
      # - Spoke must be on dive-shared Docker network (default for local dev)
      # - REDIS_PASSWORD_BLACKLIST must be set in spoke's .env (from GCP secret)
      # - For remote Hub: Update dive-hub-redis-blacklist to Hub's actual hostname
      # ==========================================================================
      BLACKLIST_REDIS_URL: redis://:${REDIS_PASSWORD_BLACKLIST}@dive-hub-redis:6379
      # Keycloak (internal Docker network URL for backend API calls)
      KEYCLOAK_URL: https://keycloak-{{INSTANCE_CODE_LOWER}}:8443
      KEYCLOAK_REALM: dive-v3-broker-{{INSTANCE_CODE_LOWER}}
      KEYCLOAK_CLIENT_ID: dive-v3-broker-{{INSTANCE_CODE_LOWER}}
      KEYCLOAK_CLIENT_SECRET: ${KEYCLOAK_CLIENT_SECRET_{{INSTANCE_CODE_UPPER}}}
      KEYCLOAK_ISSUER: {{IDP_BASE_URL}}/realms/dive-v3-broker-{{INSTANCE_CODE_LOWER}}
      # CRITICAL: Include Hub issuer for cross-instance federation
      TRUSTED_ISSUERS: {{IDP_BASE_URL}}/realms/dive-v3-broker-{{INSTANCE_CODE_LOWER}},https://keycloak-{{INSTANCE_CODE_LOWER}}:8443/realms/dive-v3-broker-{{INSTANCE_CODE_LOWER}},https://localhost:8443/realms/dive-v3-broker-usa,https://keycloak:8443/realms/dive-v3-broker-usa,https://usa-idp.dive25.com/realms/dive-v3-broker-usa
      KEYCLOAK_ADMIN_USER: admin
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD_{{INSTANCE_CODE_UPPER}}}
      # OPA
      # OPA - Use standalone OPA for policy decisions (HTTPS)
      # OPAL client pushes policies to standalone OPA
      OPA_URL: https://opa-{{INSTANCE_CODE_LOWER}}:8181
      # KAS - Local Key Access Service for this spoke
      KAS_URL: https://kas-{{INSTANCE_CODE_LOWER}}:8080
      # API URL for Swagger/OpenAPI documentation
      API_URL: {{API_URL}}
      # CORS
      NEXT_PUBLIC_BASE_URL: {{BASE_URL}}
      # CORS Configuration (Dynamic - computed from instance endpoints)
      # BEST PRACTICE: Derive allowed origins from instance configuration
      # No hardcoded URLs - all origins computed from {{BASE_URL}}, {{API_URL}}, {{IDP_URL}}
      # This ensures CORS works regardless of port assignments or domain changes
      CORS_ALLOWED_ORIGINS: {{BASE_URL}},{{API_URL}},{{IDP_URL}},https://localhost:8443,https://localhost:4000,https://localhost:3000
      # Federation mode - allows cross-instance communication
      # SECURITY NOTE: JWT authentication enforces security, not CORS
      # For production federations, set to 'true' to allow spoke-to-hub communication
      ENABLE_FEDERATION_CORS: ${ENABLE_FEDERATION_CORS:-true}
      FEDERATION_ALLOWED_ORIGINS: {{BASE_URL}},{{API_URL}},{{IDP_URL}}
      # Hub federation - use Docker internal container name for local deployment
      HUB_URL: ${HUB_URL:-https://dive-hub-backend:4000}
      SPOKE_TOKEN: ${SPOKE_TOKEN:-}  # Hub API token for heartbeat authentication
      SPOKE_CONFIG_PATH: /app/config/config.json
      DIVE_POLICY_CACHE_PATH: /app/cache/policies
      DIVE_AUDIT_QUEUE_PATH: /app/cache/audit
      # Multimedia/FFmpeg Configuration (STANAG 4774/4778)
      FFMPEG_PATH: /usr/bin/ffmpeg
      MAX_MULTIMEDIA_UPLOAD_SIZE_MB: "500"
      MAX_UPLOAD_SIZE_MB: "100"
    ports:
      - "{{BACKEND_HOST_PORT}}:4000"
    volumes:
      - ../../backend:/app
      - backend_node_modules:/app/node_modules
      - ./certs:/app/certs:rw
      - ./certs:/opt/keycloak/certs:ro
      - ./certs/ca:/app/certs/ca:ro  # Instance-specific CA for TLS trust
      - ../../NATO_Security_Policy.xml:/app/NATO_Security_Policy.xml:ro
      - ./config.json:/app/instance-config.json:ro  # Instance-specific config
      - ../../config/federation-registry.json:/app/config/federation-registry.json:ro
      # NOTE: kas-registry.json mount REMOVED in Phase 3 (MongoDB KAS Registry Migration)
      # Spokes now use MongoDB kas_registry collection as SSOT for KAS instances
      # Registration happens via: spoke_kas_register_mongodb() -> POST /api/kas/register
      - ../../policies:/app/policies:ro  # Policy metadata for /api/policies endpoint
      - ../../examples/examples:/app/examples/examples:ro  # Multi-format seeding templates
      - ./cache:/app/cache
      - backend_logs:/app/logs
    depends_on:
      postgres-{{INSTANCE_CODE_LOWER}}:
        condition: service_healthy  # NEW: Backend uses orchestration DB
      mongodb-{{INSTANCE_CODE_LOWER}}:
        condition: service_healthy
      redis-{{INSTANCE_CODE_LOWER}}:
        condition: service_healthy  # NEW: Backend uses redis for sessions
      keycloak-{{INSTANCE_CODE_LOWER}}:
        condition: service_healthy  # NEW: Backend validates JWT tokens
      opa-{{INSTANCE_CODE_LOWER}}:
        condition: service_healthy  # NEW: Explicit OPA dependency (policies loaded from volume + OPAL)
      opal-client-{{INSTANCE_CODE_LOWER}}:
        condition: service_started  # Policies loaded via volume mount (OPAL sync is bonus)
    networks:
      - dive-internal      # Internal spoke communication
      - dive-shared        # CRITICAL: Required for Hub federation and OPAL policy sync
    healthcheck:
      test: ["CMD", "curl", "-k", "-f", "https://localhost:4000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ==========================================================================
  # KAS - Key Access Service
  # ==========================================================================

  kas-{{INSTANCE_CODE_LOWER}}:
    labels:
      dive.service.class: "stretch"
      dive.service.description: "Key Access Service for TDF encrypted resources (stretch goal)"
    build:
      context: ../../kas
      dockerfile: Dockerfile
    container_name: dive-spoke-{{INSTANCE_CODE_LOWER}}-kas
    environment:
      NODE_ENV: development
      KAS_PORT: 8080
      HTTPS_ENABLED: "true"
      CERT_PATH: /opt/app/certs
      KEY_FILE: key.pem
      CERT_FILE: certificate.pem
      BACKEND_URL: https://backend-{{INSTANCE_CODE_LOWER}}:4000
      # SECURITY: Trust mkcert CA instead of disabling TLS verification
      NODE_EXTRA_CA_CERTS: /app/certs/ca/rootCA.pem
      # OPA - Use standalone OPA for policy decisions (HTTPS)
      OPA_URL: https://opa-{{INSTANCE_CODE_LOWER}}:8181
      MONGODB_URL: mongodb://admin:${MONGO_PASSWORD_{{INSTANCE_CODE_UPPER}}}@mongodb-{{INSTANCE_CODE_LOWER}}:27017/?authSource=admin&directConnection=true
      MONGODB_DATABASE: dive-v3-{{INSTANCE_CODE_LOWER}}
      REDIS_URL: redis://:${REDIS_PASSWORD_{{INSTANCE_CODE_UPPER}}}@redis-{{INSTANCE_CODE_LOWER}}:6379
      LOG_LEVEL: info
      KEYCLOAK_URL: https://keycloak-{{INSTANCE_CODE_LOWER}}:8443
      KEYCLOAK_REALM: dive-v3-broker-{{INSTANCE_CODE_LOWER}}
      KEYCLOAK_CLIENT_ID: dive-v3-broker-{{INSTANCE_CODE_LOWER}}
    ports:
      - "{{KAS_HOST_PORT}}:8080"
    depends_on:
      opa-{{INSTANCE_CODE_LOWER}}:
        condition: service_healthy
      opal-client-{{INSTANCE_CODE_LOWER}}:
        condition: service_healthy
      mongodb-{{INSTANCE_CODE_LOWER}}:
        condition: service_healthy
      redis-{{INSTANCE_CODE_LOWER}}:
        condition: service_healthy
    networks:
      - dive-internal
      - dive-shared  # For federation with hub and other spokes
    volumes:
      - ./certs:/opt/app/certs:ro
      - ../../certs/mkcert:/app/certs/ca:ro  # mkcert root CA for TLS trust
      - ../../kas/src:/app/src
      - kas_logs:/app/logs
      - ../../config:/app/config:ro
    healthcheck:
      test: ["CMD", "curl", "-kfs", "https://localhost:8080/health"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    command: npm run dev
    restart: unless-stopped

  # ==========================================================================
  # FRONTEND
  # ==========================================================================

  frontend-{{INSTANCE_CODE_LOWER}}:
    labels:
      dive.service.class: "core"
      dive.service.description: "Next.js React application with NextAuth.js"
    build:
      context: ../../frontend
      dockerfile: Dockerfile.dev
    container_name: dive-spoke-{{INSTANCE_CODE_LOWER}}-frontend
    # IMPORTANT: Use hash-based detection to reinstall when package.json changes
    # This ensures new dependencies are installed even when node_modules volume exists
    command: ["/bin/sh","-c","rm -f /app/.env.local && if [ ! -f /app/node_modules/.package-hash ] || [ \"$(md5sum /app/package.json | cut -d' ' -f1)\" != \"$(cat /app/node_modules/.package-hash 2>/dev/null)\" ]; then echo 'Package.json changed - reinstalling dependencies...' && npm install && md5sum /app/package.json | cut -d' ' -f1 > /app/node_modules/.package-hash; else echo 'Dependencies up to date'; fi && npm run dev"]
    environment:
      NODE_ENV: development
      NEXT_PUBLIC_INSTANCE: {{INSTANCE_CODE_UPPER}}
      NEXT_PUBLIC_INSTANCE_NAME: "{{INSTANCE_NAME}}"
      # Public URLs for client-side requests
      NEXT_PUBLIC_API_URL: {{API_URL}}
      NEXT_PUBLIC_KEYCLOAK_URL: {{IDP_BASE_URL}}
      NEXT_PUBLIC_KEYCLOAK_REALM: dive-v3-broker-{{INSTANCE_CODE_LOWER}}
      NEXT_PUBLIC_BACKEND_URL: {{API_URL}}
      # Internal URL for SSR requests
      BACKEND_URL: https://backend-{{INSTANCE_CODE_LOWER}}:4000
      NEXTAUTH_URL: {{BASE_URL}}
      NEXTAUTH_SECRET: ${AUTH_SECRET_{{INSTANCE_CODE_UPPER}}}
      # Database for NextAuth sessions
      DATABASE_URL: postgres://keycloak:${POSTGRES_PASSWORD_{{INSTANCE_CODE_UPPER}}}@postgres-{{INSTANCE_CODE_LOWER}}:5432/keycloak
      # Keycloak OAuth config (internal Docker network URL for SSR/API calls)
      KEYCLOAK_URL: https://keycloak-{{INSTANCE_CODE_LOWER}}:8443
      KEYCLOAK_REALM: dive-v3-broker-{{INSTANCE_CODE_LOWER}}
      KEYCLOAK_CLIENT_ID: dive-v3-broker-{{INSTANCE_CODE_LOWER}}
      KEYCLOAK_CLIENT_SECRET: ${KEYCLOAK_CLIENT_SECRET_{{INSTANCE_CODE_UPPER}}}
      # NextAuth v5 Keycloak provider
      AUTH_KEYCLOAK_ID: dive-v3-broker-{{INSTANCE_CODE_LOWER}}
      AUTH_KEYCLOAK_SECRET: ${KEYCLOAK_CLIENT_SECRET_{{INSTANCE_CODE_UPPER}}}
      AUTH_KEYCLOAK_ISSUER: {{IDP_BASE_URL}}/realms/dive-v3-broker-{{INSTANCE_CODE_LOWER}}
      AUTH_POST_LOGOUT_REDIRECT: {{BASE_URL}}

      # TLS Certificate Handling - Proper mkcert Integration (2026 Best Practice)
      # Makes Node.js fetch() API respect system CA trust store (installed via entrypoint)
      # This maintains proper TLS verification without security compromises
      # See: docs/TLS_BEST_PRACTICES_2026.md
      NODE_OPTIONS: "--use-openssl-ca"
      NODE_EXTRA_CA_CERTS: /app/certs/ca/rootCA.pem  # Legacy https module support
    ports:
      - "{{FRONTEND_HOST_PORT}}:3000"
    volumes:
      - ../../frontend:/app
      - frontend_modules:/app/node_modules
      - frontend_next:/app/.next
      - ./certs:/app/certs:ro
      - ./certs:/opt/app/certs:ro
      - ./certs/ca:/app/certs/ca:ro  # Instance-specific CA for TLS trust
    extra_hosts:
      - "localhost:host-gateway"
    depends_on:
      keycloak-{{INSTANCE_CODE_LOWER}}:
        condition: service_healthy  # NEW: NextAuth needs Keycloak realm
      backend-{{INSTANCE_CODE_LOWER}}:
        condition: service_healthy
    healthcheck:
      # FIXED: Use HTTPS (Next.js serves HTTPS only via custom server.js)
      # Test root endpoint which always exists - using curl for better HTTPS support
      test: ["CMD", "curl", "-k", "-f", "-m", "10", "https://127.0.0.1:3000/"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 60s  # INCREASED: Next.js SSR build can take 45-60s
    networks:
      - dive-internal
    restart: unless-stopped

  # ==========================================================================
  # CLOUDFLARE TUNNEL (Optional - uncomment if using dive25.com)
  # ==========================================================================

  # cloudflared-{{INSTANCE_CODE_LOWER}}:
  #   image: cloudflare/cloudflared:latest
  #   command: tunnel --no-autoupdate run --token ${TUNNEL_TOKEN}
  #   environment:
  #     TUNNEL_TOKEN: ${TUNNEL_TOKEN}
  #   networks:
  #     - dive-{{INSTANCE_CODE_LOWER}}-network
  #   restart: unless-stopped
