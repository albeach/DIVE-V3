# DIVE V3 CLI Gap Analysis

**Generated**: December 18, 2025  
**Updated**: December 19, 2025 (Phase 1 Complete)  
**Reference**: DIVE-V3-CLI-USER-GUIDE.md  
**Scope**: Documented behavior vs actual implementation

---

## Executive Summary

This gap analysis compares the DIVE V3 CLI User Guide (source of truth) against the actual codebase implementation. We identified **18 gaps** across 5 categories:

- **3 Blockers** - Prevent one-command deployment âœ… **2 RESOLVED**
- **5 High severity** - Require manual intervention âœ… **4 RESOLVED**
- **6 Medium severity** - Reduce reliability âœ… **1 RESOLVED**
- **4 Low severity** - Quality of life improvements âœ… **1 RESOLVED**

**Phase 1 Status**: 8 of 18 gaps resolved (44%)

---

## Phase 1 Resolution Summary (December 19, 2025)

### Resolved Gaps

| Gap ID | Resolution | Verification |
|--------|------------|--------------|
| GAP-002 | Added `checkpoint_create()`, `checkpoint_list()`, `cmd_rollback()` to `deploy.sh` | `./dive checkpoint create test && ./dive checkpoint list` âœ… |
| GAP-003 | Made `cmd_nuke()` fully idempotent with `docker system prune`, explicit cleanup | `./dive nuke --confirm` runs 3x without errors âœ… |
| GAP-005 | Added `--confirm` flag requirement to `cmd_nuke()` | `./dive nuke` prompts for confirmation âœ… |
| GAP-006 | Increased Keycloak timeout from 60sâ†’180s with exponential backoff | `KEYCLOAK_WAIT_TIMEOUT` configurable in `core.sh` âœ… |
| GAP-007 | Implemented `pilot_rollback()`, `pilot_checkpoint_create/list()` with GCS | `./dive --env gcp pilot rollback` available âœ… |
| GAP-008 | Created `scripts/dynamic-test-runner.sh` for Playwright discovery | Script exists and is executable âœ… |
| GAP-011 | Added CI workflow with `./dive deploy --dry-run` validation | `.github/workflows/dive-pr-checks.yml` âœ… |
| GAP-013 | Added `--json` flag to `cmd_health()` in `status.sh` | `./dive health --json | jq .` returns valid JSON âœ… |
| GAP-018 | Completed dry-run implementation for `cmd_deploy()` | `./dive --dry-run deploy` shows full plan âœ… |

### New Files Created

- `.github/workflows/dive-pr-checks.yml` - PR validation workflow
- `.github/workflows/dive-deploy.yml` - Deployment with auto-rollback
- `scripts/setup-terraform-gcs-backend.sh` - GCS state management (ready for Phase 3)
- `scripts/dynamic-test-runner.sh` - Playwright instance discovery
- `tests/e2e/local-deploy.test.sh` - Full deployment lifecycle test

### New CLI Commands

```bash
# Checkpoint management
./dive checkpoint create [name]   # Save current state
./dive checkpoint list            # List available checkpoints
./dive rollback [name]            # Restore from checkpoint

# Destructive operations now require confirmation
./dive nuke --confirm             # Destroy all resources
./dive nuke --keep-images         # Preserve Docker images

# JSON output for automation
./dive health --json              # Structured health data
./dive health --quiet             # Exit code only

# Pilot VM rollback
./dive --env gcp pilot checkpoint create
./dive --env gcp pilot rollback
```

---

## Gap Summary Table

| ID | Gap Description | Severity | Status | Impact | Proposed Fix | Effort | Phase |
|----|-----------------|----------|--------|--------|--------------|--------|-------|
| GAP-001 | Spoke user profile templates not auto-applied | **High** | ğŸ”² Pending | Locale-specific profiles require manual setup | Enhance spoke init to apply user-profile-templates | M | 2 |
| GAP-002 | No rollback on deploy failure | **Blocker** | âœ… **RESOLVED** | Stuck state after partial deploy | Add checkpoint/restore in deploy.sh | M | 1 |
| GAP-003 | `cmd_nuke` not fully idempotent | **Blocker** | âœ… **RESOLVED** | Orphaned resources on repeated runs | Add `docker system prune -af --volumes` | S | 1 |
| GAP-004 | Spoke secrets not auto-loaded | High | ğŸ”² Pending | Manual `secrets load` required | Auto-call `load_gcp_secrets` in spoke lifecycle | S | 1 |
| GAP-005 | Missing `--confirm` flag on destructive ops | High | âœ… **RESOLVED** | Accidental data loss | Add confirmation prompt to nuke/reset | S | 1 |
| GAP-006 | Keycloak wait timeout too short | High | âœ… **RESOLVED** | Startup race condition | Increase timeout, add exponential backoff | S | 1 |
| GAP-007 | No `pilot rollback` command | High | âœ… **RESOLVED** | Manual recovery required | Implement rollback in pilot.sh | M | 1 |
| GAP-008 | Missing dynamic-test-runner.sh | High | âœ… **RESOLVED** | Playwright tests fail | Create missing script or update references | M | 1 |
| GAP-009 | Missing GCP Compute Engine deploy automation | Medium | ğŸ”² Pending | Manual VM setup | Create `pilot deploy` workflow script | L | 3 |
| GAP-010 | Terraform state not shared | Medium | ğŸ”² Pending | Drift between environments | Configure GCS backend (script ready) | M | 3 |
| GAP-011 | No CI gate for local deployment | Medium | âœ… **RESOLVED** | Broken deploys reach main | Add `./dive deploy --dry-run` to PR checks | S | 4 |
| GAP-012 | Test scripts reference missing fixtures | Medium | ğŸ”² Pending | E2E tests fail | Audit and create missing fixtures | M | 5 |
| GAP-013 | No `--json` output for health commands | Medium | âœ… **RESOLVED** | Automation difficult | Add structured output option | S | 1 |
| GAP-014 | Hub seed scripts directory missing | Medium | ğŸ”² Pending | `hub seed` may fail | Create `scripts/hub-init/` or update paths | S | 1 |
| GAP-015 | No health check aggregation | Low | ğŸ”² Pending | Status requires manual inspection | Add aggregated health endpoint | S | 1 |
| GAP-016 | Hardcoded timeouts throughout | Low | ğŸ”² Pending | Not configurable | Extract to environment variables | S | 1 |
| GAP-017 | No semantic versioning for images | Low | ğŸ”² Pending | Deployment tracking difficult | Add git tag-based versioning | S | 4 |
| GAP-018 | `deploy --dry-run` not fully implemented | Low | âœ… **RESOLVED** | Preview incomplete | Complete dry-run implementation | M | 1 |

---

## Detailed Gap Analysis

### GAP-001: Spoke User Profile Templates Not Auto-Applied

**Severity**: High (downgraded from Blocker - workaround exists)  
**Category**: Keycloak Bootstrap

**User Guide Expectation**:
> `./dive spoke init <CODE>` should apply locale-specific user profile from `keycloak/user-profile-templates/`

**Actual Behavior**:
- Realm JSON is imported via `--import-realm` with default USA profile
- User profile templates exist for all 32 NATO nations in `keycloak/user-profile-templates/`
- Spoke initialization doesn't automatically apply the correct template
- Hub IdP for USA is defined; spoke-to-hub federation works via `./dive federation` commands

**Evidence**:
```bash
# keycloak/user-profile-templates/france.json has locale-specific attributes:
# "nom" (lastName), "prÃ©nom" (firstName), "courriel" (email)
# But DIVE-V3 core attributes are standardized across all:
# countryOfAffiliation, clearance, uniqueID, acpCOI
```

**Impact**:
- New spoke deployments don't have locale-specific user profiles
- Admin must manually configure user profile in Keycloak admin console
- Federation between Hub and Spokes still works (uses standardized DIVE-V3 claims)

**Proposed Fix**:
1. Enhance `./dive spoke init` to apply user profile template from `keycloak/user-profile-templates/<country>.json`
2. Create realm template generator that merges base realm JSON with locale-specific user profile
3. Add protocol mappers to normalize locale attributes to DIVE-V3 standard claims

**Effort**: Medium (3-5 days)

---

### GAP-002: No Rollback on Deploy Failure

**Severity**: Blocker  
**Category**: Deployment

**User Guide Expectation**:
> Deploy should be recoverable on failure

**Actual Behavior**:
- `cmd_deploy()` has no checkpoint mechanism
- Partial failures leave system in inconsistent state
- Only option is `nuke` and start over

**Evidence**:
```bash
# deploy.sh:19-164
cmd_deploy() {
    # 10-step process with no checkpoints
    # Each step can fail, leaving partial state
    log_step "Step 4: Stopping existing containers..."
    docker compose -f "$COMPOSE_FILE" down -v --remove-orphans
    # No way to recover if step 6+ fails
}
```

**Impact**:
- Failed deploys require full `nuke` to recover
- Data loss if volumes are removed before failure
- No visibility into last-known-good state

**Proposed Fix**:
1. Add checkpoint before destructive operations
2. Store checkpoint in `.dive-checkpoint/`
3. Implement `./dive rollback` command
4. Add `--force` to skip checkpoint

**Effort**: Medium (3-5 days)

---

### GAP-003: `cmd_nuke` Not Fully Idempotent

**Severity**: Blocker  
**Category**: Deployment

**User Guide Expectation**:
> `./dive nuke` should achieve "clean slate"

**Actual Behavior**:
- Only removes containers and named volumes
- Doesn't prune images, networks, or dangling volumes
- Running twice can still leave orphaned resources

**Evidence**:
```bash
# deploy.sh:184-207
cmd_nuke() {
    docker compose -f docker-compose.yml down -v --remove-orphans
    docker volume rm dive-v3_postgres_data ... 2>/dev/null || true
    docker volume prune -f 2>/dev/null || true
    # Missing: docker system prune -af --volumes
    # Missing: network cleanup
}
```

**Impact**:
- Orphaned networks can cause conflicts
- Stale images consume disk space
- Repeated runs may not fully reset state

**Proposed Fix**:
1. Add `docker system prune -af --volumes` with safeguards
2. Add `--confirm` flag (require explicit confirmation)
3. List what will be removed before confirming
4. Add `--keep-images` flag to preserve built images

**Effort**: Small (1-2 days)

---

### GAP-004: Spoke Secrets Not Auto-Loaded

**Severity**: High  
**Category**: Secrets Management

**User Guide Expectation**:
> `./dive --instance pol spoke up` should work without manual steps

**Actual Behavior**:
- Spoke lifecycle doesn't call `load_gcp_secrets`
- User must manually run `./dive secrets load`
- Environment variables not set for compose

**Evidence**:
```bash
# spoke.sh - spoke_up() function
# Does not call load_secrets or load_gcp_secrets
# Assumes secrets are already in environment
```

**Impact**:
- Spoke deploys fail with "missing secret" errors
- Documentation doesn't mention prerequisite
- Breaks automated deployment scripts

**Proposed Fix**:
1. Call `load_secrets` at start of `spoke_up()`
2. Pass `--instance` to `load_gcp_secrets`
3. Cache loaded secrets to avoid repeated GCP calls

**Effort**: Small (1 day)

---

### GAP-005: Missing `--confirm` Flag on Destructive Ops

**Severity**: High  
**Category**: Safety

**User Guide Expectation**:
> Destructive operations should require confirmation

**Actual Behavior**:
- `nuke` runs without confirmation
- `reset` runs without confirmation
- Easy to accidentally destroy production data

**Evidence**:
```bash
# deploy.sh:184
cmd_nuke() {
    log_warn "NUKING EVERYTHING..."
    # No confirmation prompt!
    docker compose -f docker-compose.yml down -v
}
```

**Impact**:
- Accidental data loss
- No protection in scripts/CI
- Inconsistent with Unix conventions

**Proposed Fix**:
1. Add `--confirm` or `--yes` flag requirement
2. Interactive prompt when not provided
3. `--force` flag for CI/automation (skips prompt)
4. Log warning when using `--force`

**Effort**: Small (1 day)

---

### GAP-006: Keycloak Wait Timeout Too Short

**Severity**: High  
**Category**: Reliability

**User Guide Expectation**:
> Stack should reliably start on any machine

**Actual Behavior**:
- `wait_for_keycloak()` has 60s timeout
- Cold starts can take 90-120s on slower machines
- First-time realm import adds delay

**Evidence**:
```bash
# core.sh:182-199
wait_for_keycloak() {
    local retries=12 # 12 * 5s = 60s
    # Timeout on slower machines
}
```

**Impact**:
- Intermittent startup failures
- CI failures on cold runners
- User frustration on first deploy

**Proposed Fix**:
1. Increase default to 180s
2. Add exponential backoff
3. Make timeout configurable via `KEYCLOAK_WAIT_TIMEOUT`
4. Add health check for realm import completion

**Effort**: Small (1 day)

---

### GAP-007: No `pilot rollback` Command

**Severity**: High  
**Category**: GCP Deployment

**User Guide Expectation**:
> `./dive --env gcp pilot rollback` for recovery

**Actual Behavior**:
- Command not implemented in `pilot.sh`
- No checkpoint mechanism for GCP state
- Manual recovery required

**Evidence**:
```bash
# pilot.sh - No rollback function exists
# Only has: up, down, status, logs, ssh, deploy, reset
```

**Impact**:
- Failed GCP deploys require manual intervention
- No way to revert to previous state
- Production risk

**Proposed Fix**:
1. Create Terraform state snapshots before apply
2. Store Docker volume backups in GCS
3. Implement `pilot rollback` using snapshots
4. Add `--to-version` flag for specific rollback

**Effort**: Medium (3-5 days)

---

### GAP-008: Missing dynamic-test-runner.sh

**Severity**: High  
**Category**: Testing

**User Guide Expectation**:
> `./dive test playwright` runs dynamic tests

**Actual Behavior**:
- References `scripts/dynamic-test-runner.sh`
- File does not exist
- Playwright tests fail to run

**Evidence**:
```bash
# test.sh:320-333
test_playwright() {
    local playwright_script="${DIVE_ROOT}/scripts/dynamic-test-runner.sh"
    if [ ! -f "$playwright_script" ]; then
        log_error "Dynamic test runner not found: ${playwright_script}"
        # Always fails!
    }
}
```

**Impact**:
- `./dive test playwright` always fails
- Dynamic instance testing unavailable
- User Guide documents non-existent feature

**Proposed Fix**:
1. Create `scripts/dynamic-test-runner.sh`
2. OR update `test.sh` to use existing Playwright config
3. Add test to CI to catch missing scripts

**Effort**: Medium (2-3 days)

---

### GAP-009: Missing GCP Compute Engine Deploy Automation

**Severity**: Medium  
**Category**: GCP Deployment

**User Guide Expectation**:
> `./dive --env gcp pilot deploy` provisions complete stack

**Actual Behavior**:
- `pilot deploy` exists but is incomplete
- VM provisioning not automated
- Requires manual GCP console steps

**Evidence**:
```bash
# pilot.sh - pilot_deploy() references terraform
# but terraform/gcp/ or terraform/compute/ doesn't exist
```

**Impact**:
- Manual VM creation required
- Inconsistent environments
- Slow onboarding for new instances

**Proposed Fix**:
1. Create `terraform/compute/` for VM provisioning
2. Add startup script for Docker installation
3. Integrate with `pilot deploy` workflow
4. Add SSH key management

**Effort**: Large (5-10 days)

---

### GAP-010: Terraform State Not Shared

**Severity**: Medium  
**Category**: Infrastructure

**User Guide Expectation**:
> Multiple operators can manage infrastructure

**Actual Behavior**:
- Terraform state stored locally
- No locking mechanism
- State drift between operators

**Evidence**:
```
terraform/pilot/terraform.tfstate  # Local file, not gitignored
terraform/spoke/terraform.tfstate.d/  # Workspace state, local
```

**Impact**:
- Concurrent applies can corrupt state
- No visibility into who changed what
- Recovery requires manual state sync

**Proposed Fix**:
1. Create GCS bucket `gs://dive25-tfstate`
2. Configure `backend "gcs"` in all modules
3. Enable state locking
4. Add state versioning

**Effort**: Medium (2-3 days)

---

### GAP-011: No CI Gate for Local Deployment

**Severity**: Medium  
**Category**: CI/CD

**User Guide Expectation**:
> PRs should validate deployment works

**Actual Behavior**:
- CI runs tests but not deployment validation
- Broken deploys can merge to main
- Only discovered in production

**Evidence**:
```yaml
# .github/workflows/ci-pr.yml
# No ./dive deploy --dry-run step
```

**Impact**:
- Deployment breaks discovered late
- Increased production incidents
- Manual testing burden

**Proposed Fix**:
1. Add `./dive deploy --dry-run` job to PR checks
2. Validate compose configuration
3. Check secret availability
4. Block merge on failure

**Effort**: Small (1 day)

---

### GAP-012: Test Scripts Reference Missing Fixtures

**Severity**: Medium  
**Category**: Testing

**User Guide Expectation**:
> All tests should pass

**Actual Behavior**:
- Some test fixtures not present
- `tests/fixtures/federation/` partially populated
- Tests fail with file not found errors

**Evidence**:
```
tests/fixtures/federation/spoke-configs/  # Only 3 of 32 countries
tests/fixtures/federation/certificates/   # Empty directory
```

**Impact**:
- E2E tests fail intermittently
- Incomplete test coverage
- False negatives in CI

**Proposed Fix**:
1. Audit all test scripts for fixture dependencies
2. Generate missing fixtures
3. Add fixture validation to CI
4. Document fixture generation process

**Effort**: Medium (2-3 days)

---

### GAP-013: No `--json` Output for Health Commands

**Severity**: Medium  
**Category**: Observability

**User Guide Expectation**:
> `./dive health` should be scriptable

**Actual Behavior**:
- Only human-readable output
- Exit codes not consistent
- Can't integrate with monitoring

**Evidence**:
```bash
# status.sh - cmd_health()
# Outputs colored text, no JSON option
echo -e "  ${GREEN}âœ“${NC} $service healthy"
```

**Impact**:
- Can't use in scripts
- No integration with monitoring tools
- Manual parsing required

**Proposed Fix**:
1. Add `--json` flag to health commands
2. Return structured JSON with service status
3. Use consistent exit codes (0=healthy, 1=degraded, 2=down)
4. Add `--quiet` for exit-code-only

**Effort**: Small (1-2 days)

---

### GAP-014: Hub Seed Scripts Directory Missing

**Severity**: Medium  
**Category**: Data Management

**User Guide Expectation**:
> `./dive hub seed` populates test data

**Actual Behavior**:
- References `scripts/hub-init/` directory
- Directory may not exist or be incomplete
- Seed functions fall back with warnings

**Evidence**:
```bash
# hub.sh:1577-1596
hub_seed() {
    local SEED_SCRIPTS_DIR="${DIVE_ROOT}/scripts/hub-init"
    if [ ! -d "$SEED_SCRIPTS_DIR" ]; then
        log_error "Hub seed scripts not found at $SEED_SCRIPTS_DIR"
        return 1
    }
}
```

**Impact**:
- `hub seed` fails on fresh checkouts
- ABAC testing blocked
- User confusion

**Proposed Fix**:
1. Create `scripts/hub-init/` with seed scripts
2. OR update path to existing seed location
3. Add CI check for required scripts

**Effort**: Small (1 day)

---

### GAP-015: No Health Check Aggregation

**Severity**: Low  
**Category**: Observability

**User Guide Expectation**:
> Single command shows overall system health

**Actual Behavior**:
- Each service checked individually
- No aggregate health score
- No single endpoint for load balancers

**Proposed Fix**:
1. Add health aggregation service to compose
2. OR add aggregate calculation in `cmd_health`
3. Return single pass/fail for all services

**Effort**: Small (1 day)

---

### GAP-016: Hardcoded Timeouts Throughout

**Severity**: Low  
**Category**: Configurability

**User Guide Expectation**:
> System adapts to different environments

**Actual Behavior**:
- Timeouts hardcoded (60s, 90s, 180s)
- No way to adjust for slow networks
- CI and production use same values

**Proposed Fix**:
1. Extract timeouts to environment variables
2. Document in user guide
3. Provide sensible defaults

**Effort**: Small (1 day)

---

### GAP-017: No Semantic Versioning for Images

**Severity**: Low  
**Category**: Release Management

**User Guide Expectation**:
> Deployments should be traceable

**Actual Behavior**:
- Images tagged `latest` or `test`
- No git SHA or version tags
- Can't identify what's deployed

**Proposed Fix**:
1. Tag images with git SHA
2. Add semantic version from git tags
3. Include build metadata

**Effort**: Small (1 day)

---

### GAP-018: `deploy --dry-run` Not Fully Implemented

**Severity**: Low  
**Category**: Deployment

**User Guide Expectation**:
> `--dry-run` shows complete plan

**Actual Behavior**:
- Some steps skip in dry-run
- Output inconsistent
- Some operations still execute

**Proposed Fix**:
1. Audit all `cmd_deploy` steps for dry-run
2. Ensure no side effects in dry-run mode
3. Show complete plan before execution

**Effort**: Medium (2-3 days)

---

## Priority Matrix

```
                    IMPACT
                    High        Medium      Low
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    High   â”‚ GAP-001    GAP-009    GAP-017    â”‚
           â”‚ GAP-002    GAP-010               â”‚
  EFFORT   â”‚ GAP-007    GAP-012               â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    Low    â”‚ GAP-003    GAP-011    GAP-015    â”‚
           â”‚ GAP-004    GAP-013    GAP-016    â”‚
           â”‚ GAP-005    GAP-014    GAP-018    â”‚
           â”‚ GAP-006    GAP-008               â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Recommended Order**:
1. **Phase 1** (Week 1): GAP-003, GAP-005, GAP-004, GAP-006, GAP-002, GAP-013, GAP-014
2. **Phase 2** (Week 2): GAP-001
3. **Phase 3** (Week 3): GAP-010, GAP-007, GAP-009
4. **Phase 4** (Week 4): GAP-011, GAP-017
5. **Phase 5** (Week 5): GAP-008, GAP-012, GAP-015, GAP-016, GAP-018

---

## Appendix: Verification Commands

```bash
# Verify GAP-001 (IdPs exist)
./dive exec keycloak /opt/keycloak/bin/kcadm.sh get identity-provider/instances -r dive-v3-broker

# Verify GAP-003 (clean nuke)
./dive nuke --confirm && docker ps -a && docker volume ls && docker network ls

# Verify GAP-004 (auto secrets)
unset POSTGRES_PASSWORD && ./dive --instance fra spoke up

# Verify GAP-013 (JSON output)
./dive health --json | jq '.services[] | select(.healthy == false)'

# Verify GAP-010 (remote state)
cd terraform/pilot && terraform state list
```
